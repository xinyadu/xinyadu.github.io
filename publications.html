<!doctype html>
<html lang="en">

	<head>
		<title>Xinya Du</title>

		<!-- Required meta tags -->
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

		<!-- Bootstrap CSS -->
		<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
		<link rel="stylesheet" href="css/my.css">

		<!-- from bootstrap -->
		<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
		<script src="https://cdn.jsdelivr.net/npm/popper.js@1.14.6/dist/umd/popper.min.js" integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut" crossorigin="anonymous"></script>
		<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.2.1/dist/js/bootstrap.min.js" integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k" crossorigin="anonymous"></script>

		<!-- from CDQ -->
		<script>
			function copy(dest, source) {
				if(dest.source == source) {
				dest.innerHTML = "";
				dest.source = null;
				dest.style.width="0px";
				dest.style.border = "";
				dest.style.padding = "0px";
				}
				else {
				dest.innerHTML = source.innerHTML;
				dest.source = source;
				dest.style.width = "800px";
				dest.style.padding = "10px";
				dest.style.border = "2px dotted gray";
				dest.style.background = "#F5F5F5";
				dest.style.margin = "10px";
			  }
			  dest.blur();
			}
		</script>	
	</head>


 	<body>
	    <!-- Navigation -->
	    <nav class="navbar fixed-top navbar-expand-lg navbar-light" style="background-color: #6495ED;">
	        <div class="container">
	            <!-- <a class="navbar-brand" href="#">Navbar</a> -->
	            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
	                <span class="navbar-toggler-icon"></span>
	            </button>

	            <div class="collapse navbar-collapse" id="navbarSupportedContent">

			        <ul class="navbar-nav mr-auto">
			        	<!-- <li class="nav-item active"> -->
			            	<!-- <a class="nav-link" href="#"><b>Home</b> <span class="sr-only">(current)</span></a> -->
			            <!-- </li> -->
						<li class="nav-item">
			            	<a class="nav-link" href="index.html"><b>Home</b></a>
			            </li>
			            <li class="nav-item">
			            	<a class="nav-link" href="index.html#research_interest">Research Interest</a>
			            </li>
			            <li class="nav-item">
			            	<a class="nav-link" href="publications.html">Publications</a>
			            </li>
			            <li class="nav-item">
			            	<a class="nav-link" href="index.html#research_interest">Advising</a>
			            </li>
			            <li class="nav-item">
			            	<a class="nav-link" href="teaching_service.html">Teaching&amp;Service</a>
			            </li>
		          	</ul>
	            </div>  
	    	</div>
	    </nav>
	    <br><br>

 		<div class="container mt-4">
         	<!-- Publications -->
         	<hr>
         	<section name="publications" id="publications" style="scroll-margin-top: 55px">
	        	<h4>Publications <a href="https://scholar.google.com/citations?user=R-lKQqkAAAAJ&hl=en"> (Google Scholar)</a> </h4>
			
				<script>
				        paper_count = 0

				        function add_paper(title, authors, conference, link, bib, abstract, arxiv_link, code, data, slides, talk, msg) {
				            list_entry = "<li style=\"font-size:15px\">"
				            if (link != null)
				                list_entry += "<a href=\"" + link + "\">"
				            list_entry += "<b>" + title + "</b>"
				            if (link != null)
				                list_entry += "</a>"
				            list_entry += "<br>" + authors + "<br>" + conference + "</li>"

				            if (bib != null) {
				                list_entry += "<div id=\"bib" + paper_count + "\" style=\"display:none\">" + bib + "</div>"
				                list_entry += "<a href=\"javascript:copy(div" + paper_count + ",bib" + paper_count + ")\"> <span class=\"btn-sm btn-info\">bib</span></a>"
				            }

				            if (abstract != null) {
				                list_entry += "<div id=\"abstract" + paper_count + "\" style=\"display:none\">" + abstract + "</div>"
				                list_entry += " <a href=\"javascript:copy(div" + paper_count + ",abstract" + paper_count + ")\"> <span class=\"btn-sm btn-info\">abstract</span></a>"
				                // list_entry += " <a target=\"_blank\" href=\"javascript:copy(div" + paper_count + ",abstract" + paper_count + ")\" class=\"btn-sm btn-info\">abstract</a>"
				            }
				            if (arxiv_link != null)
				                list_entry += " <a target=\"_blank\" href=\"" + arxiv_link + "\" class=\"btn-sm btn-info\">arXiv</a>"

				            if (code != null)
				                list_entry += " <a target=\"_blank\" href=\"" + code + "\" class=\"btn-sm btn-info\">code</a>"

				            if (data != null)
				                list_entry += " <a target=\"_blank\" href=\"" + data + "\" class=\"btn-sm btn-info\">data</a>"

				            if (slides != null)
				                list_entry += " <a target=\"_blank\" href=\"" + slides + "\" class=\"btn-sm btn-info\">presentation</a>"

				            if (talk != null)
				                list_entry += " <a target=\"_blank\" href=\"" + talk + "\" class=\"btn-sm btn-info\">talk</a>"

				            list_entry += "<br>"

				            if (msg != null)
				                list_entry += "<i>" + msg + "</i>"

				            list_entry += "<div id=\"div" + paper_count + "\" style=\"font-size:15px\"></div><br>"

				            document.write(list_entry)

				            paper_count += 1
				        }

				        // ---------------------------------- 2022 ------------------------------------------------------------------------------------------------------
				        document.write("<h5>2022</h5>")
				        document.write("<ul style=\"list-style: none;\">")
				        add_paper("Dynamic Global Memory for Document-level Argument Extraction",
				            "<b>Xinya Du</b>, Sha Li, Heng Ji", // authors
				            "Association for Computational Linguistics and the International Joint Conference on Natural Language Processing (ACL-IJCNLP) 2022", // conference
				            "https://aclanthology.org/2022.acl-long.361/", // link
				            "@InProceedings{du2022dynamic,<br>" + // bib
				            "&nbsp;&nbsp;&nbsp;title={Dynamic Global Memory for Document-level Argument Extraction},<br>" +
				            "&nbsp;&nbsp;&nbsp;author={Du, Xinya and Li, Sha and Ji, Heng},<br>" +
				            "&nbsp;&nbsp;&nbsp;booktitle={Association for Computational Linguistics (ACL)},<br>" +
				            "&nbsp;&nbsp;&nbsp;year={2022}<br>}",
				            "Extracting informative arguments of events from news articles is a challenging problem in information extraction, which requires a global understanding of each document. While recent work on document-level extraction has gone beyond single-sentence and increased the cross-sentence inference capability of end-to-end models, they are still restricted by certain input sequence length constraints and usually ignore the global context between events. To tackle this issue, we introduce a new global neural generation-based framework for document-level event argument extraction by constructing a document memory store to record the contextual event information and leveraging it to implicitly and explicitly help with decoding of arguments for later events. Empirical results show that our framework outperforms prior methods substantially and it is more robust to adversarially annotated examples with our constrained decoding design.", // abstract
				            null, // arXiv_link
				            "https://github.com/xinyadu/memory_docie", // code
				            // "abc", // data
				            // "abc" // pre
				        )

				        add_paper("Automatic Error Analysis for Document-level Information Extraction",
				            "Aliva Das<sup>*</sup>, <b>Xinya Du</b><sup>*</sup>, Barry Wang<sup>*</sup>, Jiayuan Gu, Kejian Shi, Thomas Porter, Claire Cardie", // authors
				            "ACL 2022", // conference
				            "https://aclanthology.org/2022.acl-long.274.pdf", // link
				            "@InProceedings{das2022error,<br>" + // bib
				            "&nbsp;&nbsp;&nbsp;title={Automatic Error Analysis for Document-level Information Extraction},<br>" +
				            "&nbsp;&nbsp;&nbsp;author={Das, Aliva and Du, Xinya  and Wang, Barry  and Shi, Kejian  and Gu, Jiayuan  and Porter, Thomas  and Cardie, Claire,<br>" +
				            "&nbsp;&nbsp;&nbsp;booktitle={Association for Computational Linguistics (ACL)},<br>" +
				            "&nbsp;&nbsp;&nbsp;year={2022}<br>}",
				            "Document-level information extraction (IE) tasks have recently begun to be revisited in earnest using the end-to-end neural network techniques that have been successful on their sentence-level IE counterparts. Evaluation of the approaches, however, has been limited in a number of dimensions. In particular, the precision/recall/F1 scores typically reported provide few insights on the range of errors the models make. We build on the work of Kummerfeld and Klein (2013) to propose a transformation-based framework for automating error analysis in document-level event and (N-ary) relation extraction. We employ our framework to compare two state-of-the-art document-level template-filling approaches on datasets from three domains; and then, to gauge progress in IE since its inception 30 years ago, vs. four systems from the MUC-4 (1992) evaluation.", // abstract
				            null, // arXiv_link
				            "https://github.com/icejinx33/auto-err-template-fill", // code
				        )
				        add_paper("RESIN-11: Schema-guided Event Prediction for 11 Newsworthy Scenarios",
				            "<b>Xinya Du</b>, Zixuan Zhang, Sha Li, Heng Ji and RESIN team", // authors
				            "NAACL 2022 (demo)", // conference
				            "https://blender.cs.illinois.edu/paper/resin2022.pdf", // link
				            null, //bib
				            null, //abs
				            null, //arxiv
				            "https://github.com/RESIN-KAIROS/RESIN-11",//code
				            null, //data
				            "http://18.221.187.153:11000/kairos",  //pre
				        )
				        document.write("</ul>")


				        // ---------------------------------- 2021 ------------------------------------------------------------------------------------------------------
				        document.write("<h5>2021</h5>")
				        document.write("<ul style=\"list-style: none;\">")
				        add_paper("Towards More Intelligent Extraction of Information from Documents",
				            "<b>Xinya Du</b>", // authors
				            "PhD Dissertation", // conference
				            "https://search.proquest.com/openview/ff090eb702b98c8e4562865ddb5e7f81/1?pq-origsite=gscholar&cbl=18750&diss=y", // link
						    "@article{du2021towards,<br>" +
 							"&nbsp;&nbsp;&nbsp;title={Towards More Intelligent Extraction of Information from Documents},<br>" + 
  						    "&nbsp;&nbsp;&nbsp;author={Du, Xinya},<br>" + 
  						    "&nbsp;&nbsp;&nbsp;year={2021},<br>" + 
  						    "&nbsp;&nbsp;&nbsp;school={Cornell University}<br>}",
				        )
				        add_paper("Template Filling with Generative Transformers",
				            "<b>Xinya Du</b>, Alexander M. Rush, Heng Ji", // authors
				            "NAACL 2021 (short)", // conference
				            "https://aclanthology.org/2021.naacl-main.70", // link
				            null,
				            // "@InProceedings{du2022dynamic,<br>" + // bib
				            // "&nbsp;&nbsp;&nbsp;title={Dynamic Global Memory for Document-level Argument Extraction},<br>" +
				            // "&nbsp;&nbsp;&nbsp;author={Du, Xinya and Li, Sha and Ji, Heng},<br>" +
				            // "&nbsp;&nbsp;&nbsp;booktitle={Association for Computational Linguistics (ACL)},<br>" +
				            // "&nbsp;&nbsp;&nbsp;year={2022}<br>}",
				            "Template filling is generally tackled by a pipeline of two separate supervised systems – one for role-filler extraction and another for template/event recognition. Since pipelines consider events in isolation, they can suffer from error propagation. We introduce a framework based on end-to-end generative transformers for this task (i.e., GTT). It naturally models the dependence between entities both within a single event and across the multiple events described in a document. Experiments demonstrate that this framework substantially outperforms pipeline-based approaches, and other neural end-to-end baselines that do not model between-event dependencies. We further show that our framework specifically improves performance on documents containing multiple events.", // abstract
				            null, // arXiv_link
				            "https://github.com/xinyadu/gtt", // code
				        )
				        add_paper("GRIT: Generative Role-filler Transformers for <strong>Document-level</strong> Event Entity Extraction",
				            "<b>Xinya Du</b>, Alexander M. Rush, Heng Ji", // authors
				            "EACL 2021", // conference
				            "https://aclanthology.org/2021.eacl-main.52", // link
				            null,
				            // "@InProceedings{du2022dynamic,<br>" + // bib
				            // "&nbsp;&nbsp;&nbsp;title={Dynamic Global Memory for Document-level Argument Extraction},<br>" +
				            // "&nbsp;&nbsp;&nbsp;author={Du, Xinya and Li, Sha and Ji, Heng},<br>" +
				            // "&nbsp;&nbsp;&nbsp;booktitle={Association for Computational Linguistics (ACL)},<br>" +
				            // "&nbsp;&nbsp;&nbsp;year={2022}<br>}",
				            "We revisit the classic problem of document-level role-filler entity extraction (REE) for template filling. We argue that sentence-level approaches are ill-suited to the task and introduce a generative transformer-based encoder-decoder framework (GRIT) that is designed to model context at the document level: it can make extraction decisions across sentence boundaries; is implicitly aware of noun phrase coreference structure, and has the capacity to respect cross-role dependencies in the template structure. We evaluate our approach on the MUC-4 dataset, and show that our model performs substantially better than prior work. We also show that our modeling choices contribute to model performance, e.g., by implicitly capturing linguistic knowledge such as recognizing coreferent entity mentions.", // abstract
				            null, // arXiv_link
				            "https://github.com/xinyadu/grit_doc_event_entity", // code
				        )
				        add_paper("Few-shot Intent Classification and Slot Filling with Retrieved Examples",
				            "Dian Yu, Luheng He, Yuan Zhang, <strong>Xinya Du</strong>, Panupong Pasupat, Qi Li", // authors
				            "NAACL 2021", // conference
				            "https://aclanthology.org/2021.naacl-main.59/", // link
				            null,
				            // "@InProceedings{du2022dynamic,<br>" + // bib
				            // "&nbsp;&nbsp;&nbsp;title={Dynamic Global Memory for Document-level Argument Extraction},<br>" +
				            // "&nbsp;&nbsp;&nbsp;author={Du, Xinya and Li, Sha and Ji, Heng},<br>" +
				            // "&nbsp;&nbsp;&nbsp;booktitle={Association for Computational Linguistics (ACL)},<br>" +
				            // "&nbsp;&nbsp;&nbsp;year={2022}<br>}",
				            "Few-shot learning arises in important practical scenarios, such as when a natural language understanding system needs to learn new semantic labels for an emerging, resource-scarce domain. In this paper, we explore retrieval-based methods for intent classification and slot filling tasks in few-shot settings. Retrieval-based methods make predictions based on labeled examples in the retrieval index that are similar to the input, and thus can adapt to new domains simply by changing the index without having to retrain the model. However, it is non-trivial to apply such methods on tasks with a complex label space like slot filling. To this end, we propose a span-level retrieval method that learns similar contextualized representations for spans with the same label via a novel batch-softmax objective. At inference time, we use the labels of the retrieved spans to construct the final structure with the highest aggregated score. Our method outperforms previous systems in various few-shot settings on the CLINC and SNIPS benchmarks.", // abstract
				            "https://arxiv.org/abs/2104.05763", // arXiv_link
				            // "https://github.com/xinyadu/grit_doc_event_entity", // code
				        )
				        add_paper("QA-Driven Zero-shot Slot Filling with Weak Supervision Pretraining",
				            "<strong>Xinya Du</strong>, Luheng He, Qi Li, Dian Yu, Panupong Pasupat and Yuan Zhang", // authors
				            "ACL 2021", // conference
				            "https://aclanthology.org/2021.acl-short.83.pdf", // link
				            null,
				            // "@InProceedings{du2022dynamic,<br>" + // bib
				            // "&nbsp;&nbsp;&nbsp;title={Dynamic Global Memory for Document-level Argument Extraction},<br>" +
				            // "&nbsp;&nbsp;&nbsp;author={Du, Xinya and Li, Sha and Ji, Heng},<br>" +
				            // "&nbsp;&nbsp;&nbsp;booktitle={Association for Computational Linguistics (ACL)},<br>" +
				            // "&nbsp;&nbsp;&nbsp;year={2022}<br>}",
				            "Slot-filling is an essential component for build- ing task-oriented dialog systems. In this work, we focus on the zero-shot slot-filling problem, where the model needs to predict slots and their values, given utterances from new do- mains without training on the target domain. Prior methods directly encode slot descrip- tions to generalize to unseen slot types. How- ever, raw slot descriptions are often ambigu- ous and do not encode enough semantic in- formation, limiting the models’ zero-shot ca- pability. To address this problem, we intro- duce QA-driven slot filling (QASF), which ex- tracts slot-filler spans from utterances with a span-based QA model. We use a linguistically motivated questioning strategy to turn descrip- tions into questions, allowing the model to gen- eralize to unseen slot types. Moreover, our QASF model can benefit from weak supervi- sion signals from QA pairs synthetically gen- erated from unlabeled conversations. Our full system substantially outperforms baselines by over 5% on the SNIPS benchmark.", // abstract
				            // "https://arxiv.org/abs/2104.05763", // arXiv_link
				            // "https://github.com/xinyadu/grit_doc_event_entity", // code
				        )

				        document.write("</ul>")

						// ---------------------------------- 2020 ------------------------------------------------------------------------------------------------------
				        document.write("<h5>2020</h5>")
				        document.write("<ul style=\"list-style: none;\">")
				        add_paper("Event Extraction by Answering (Almost) Natural <strong>Questions</strong>",
				            "<b>Xinya Du</b>, Claire Cardie", // authors
				            "EMNLP 2020", // conference
				            "https://aclanthology.org/2020.emnlp-main.49/", // link
				            // null,
				            "@Inproceedings{du2022eeqa,<br>" + // bib
				            "&nbsp;&nbsp;&nbsp;title={Event Extraction by Answering (Almost) Natural Questions},<br>" +
				            "&nbsp;&nbsp;&nbsp;author={Du, Xinya and Cardie, Claire},<br>" +
				            "&nbsp;&nbsp;&nbsp;booktitle={EMNLP)},<br>" +
				            "&nbsp;&nbsp;&nbsp;year={2022}<br>}",
				            "The problem of event extraction requires detecting the event trigger and extracting its corresponding arguments. Existing work in event argument extraction typically relies heavily on entity recognition as a preprocessing/concurrent step, causing the well-known problem of error propagation. To avoid this issue, we introduce a new paradigm for event extraction by formulating it as a question answering (QA) task that extracts the event arguments in an end-to-end manner. Empirical results demonstrate that our framework outperforms prior methods substantially; in addition, it is capable of extracting event arguments for roles not seen at training time (i.e., in a zero-shot learning setting).", // abstract
				            "https://arxiv.org/abs/2004.13625", // arXiv_link
				            "https://github.com/xinyadu/eeqa", // code
				        )
				        add_paper("Improving Event Duration Prediction via Time-aware Pre-training",
				            "Zonglin Yang, <strong>Xinya Du</strong>, Alexander Rush, Claire Cardie", // authors
				            "EMNLP 2020 (Findings)", // conference
				            "https://aclanthology.org/2020.findings-emnlp.302.pdf", // link
				            null,
				            // "@Inproceedings{du2022eeqa,<br>" + // bib
				            // "&nbsp;&nbsp;&nbsp;title={Dynamic Global Memory for Document-level Argument Extraction},<br>" +
				            // "&nbsp;&nbsp;&nbsp;author={Du, Xinya and Cardie, Claire},<br>" +
				            // "&nbsp;&nbsp;&nbsp;booktitle={EMNLP)},<br>" +
				            // "&nbsp;&nbsp;&nbsp;year={2022}<br>}",
				            "End-to-end models in NLP rarely encode external world knowledge about length of time. We introduce two effective models for duration prediction, which incorporate external knowledge by reading temporal-related news sentences (time-aware pre-training). Specifically, one model predicts the range/unit where the duration value falls in (R-PRED); and the other predicts the exact duration value (E-PRED). Our best model – E-PRED, substantially outperforms previous work, and captures duration information more accurately than R-PRED. We also demonstrate our models are capable of duration prediction in the unsupervised setting, outperforming the baselines.", // abstract
				            // "https://arxiv.org/abs/2004.13625", // arXiv_link
				            // "https://github.com/xinyadu/eeqa", // code
				        )
				        add_paper("Document-Level <em>Event Role Filler</em> Extraction Using Multi-Granularity Contextualized Encoding",
				            "<strong>Xinya Du</strong>, Claire Cardie", // authors
				            "ACL 2020", // conference
				            "https://aclanthology.org/2020.acl-main.714/", // link
				            // null,
				            "@Inproceedings{du2020document,<br>" + // bib
				            "&nbsp;&nbsp;&nbsp;title={Document-Level Event Role Filler Extraction Using Multi-Granularity Contextualized Encoding},<br>" +
				            "&nbsp;&nbsp;&nbsp;author={Du, Xinya and Cardie, Claire},<br>" +
				            "&nbsp;&nbsp;&nbsp;booktitle={ACL},<br>" +
				            "&nbsp;&nbsp;&nbsp;year={2020}<br>}",
				            "Few works in the literature of event extraction have gone beyond individual sentences to make extraction decisions. This is problematic when the information needed to recognize an event argument is spread across multiple sentences. We argue that document-level event extraction is a difficult task since it requires a view of a larger context to determine which spans of text correspond to event role fillers. We first investigate how end-to-end neural sequence models (with pre-trained language model representations) perform on document-level role filler extraction, as well as how the length of context captured affects the models’ performance. To dynamically aggregate information captured by neural representations learned at different levels of granularity (e.g., the sentence- and paragraph-level), we propose a novel multi-granularity reader. We evaluate our models on the MUC-4 event extraction dataset, and show that our best system performs substantially better than prior work. We also report findings on the relationship between context length and neural model performance on the task.", // abstract
				            null,
				            // "https://arxiv.org/abs/2004.13625", // arXiv_link
				            "https://github.com/xinyadu/doc_event_role", // code
				        )
				        add_paper("Leveraging Structured Metadata for Improving Question Answering on the Web",
				            "<strong>Xinya Du</strong>, Adam Fourney, Robert Sim, Paul Bennett, Claire Cardie, Ahmed Hassan Awadallah", // authors
				            "AACL 2020", // conference
				            "https://aclanthology.org/2020.aacl-main.55/", // link
				            null,
				            // "@Inproceedings{du2020document,<br>" + // bib
				            // "&nbsp;&nbsp;&nbsp;title={Document-Level Event Role Filler Extraction Using Multi-Granularity Contextualized Encoding},<br>" +
				            // "&nbsp;&nbsp;&nbsp;author={Du, Xinya and Cardie, Claire},<br>" +
				            // "&nbsp;&nbsp;&nbsp;booktitle={ACL},<br>" +
				            // "&nbsp;&nbsp;&nbsp;year={2020}<br>}",
				            "We show that leveraging metadata information from web pages can improve the performance of models for answer passage selection/reranking. We propose a neural passage selection model that leverages metadata information with a fine-grained encoding strategy, which learns the representation for metadata predicates in a hierarchical way. The models are evaluated on the MS MARCO (Nguyen et al., 2016) and Recipe-MARCO datasets. Results show that our models significantly outperform baseline models, which do not incorporate metadata. We also show that the fine-grained encoding’s advantage over other strategies for encoding the metadata.", // abstract
				            null,
				            // "https://arxiv.org/abs/2004.13625", // arXiv_link
				            // "https://github.com/xinyadu/doc_event_role", // code
				        )
				        document.write("</ul>")


						// ---------------------------------- 2019 ------------------------------------------------------------------------------------------------------
				        document.write("<h5>2019</h5>")
				        document.write("<ul style=\"list-style: none;\">")
				        add_paper("Be Consistent! Improving Procedural Text Comprehension using Label Consistency",
				            "<b>Xinya Du</b>, Bhavana Dalvi, Niket Tandon, Antoine Bosselut, Wen-tau Yih, Peter Clark, Claire Cardie", // authors
				            "NAACL 2019", // conference
				            "https://aclanthology.org/N19-1244/", // link
				            // null,
				            "@Inproceedings{du2019consistency,<br>" + // bib
				            "&nbsp;&nbsp;&nbsp;title={Be Consistent! Improving Procedural Text Comprehension using Label Consistency},<br>" +
				            "&nbsp;&nbsp;&nbsp;author={Du, Xinya  and Dalvi, Bhavana  and Tandon, Niket  and Bosselut, Antoine  and Yih, Wen-tau  and Clark, Peter  and Cardie, Claire},<br>" +
				            "&nbsp;&nbsp;&nbsp;booktitle={NAACL},<br>" +
				            "&nbsp;&nbsp;&nbsp;year={2019}<br>}",
				            "Our goal is procedural text comprehension, namely tracking how the properties of entities (e.g., their location) change with time given a procedural text (e.g., a paragraph about photosynthesis, a recipe). This task is challenging as the world is changing throughout the text, and despite recent advances, current systems still struggle with this task. Our approach is to leverage the fact that, for many procedural texts, multiple independent descriptions are readily available, and that predictions from them should be consistent (label consistency). We present a new learning framework that leverages label consistency during training, allowing consistency bias to be built into the model. Evaluation on a standard benchmark dataset for procedural text, ProPara (Dalvi et al., 2018), shows that our approach significantly improves prediction performance (F1) over prior state-of-the-art systems.", // abstract
				            null,
				            // "https://arxiv.org/abs/2004.13625", // arXiv_link
				            // "https://github.com/xinyadu/doc_event_role", // code
				        )

				        document.write("</ul>")

						// ---------------------------------- 2018 ------------------------------------------------------------------------------------------------------
				        document.write("<h5>2018</h5>")
				        document.write("<ul style=\"list-style: none;\">")
				        add_paper("Harvesting Paragraph-level Question-Answer Pairs from Wikipedia",
				            "<b>Xinya Du</b>, Claire Cardie", // authors
				            "ACL 2018", // conference
				            "https://aclanthology.org/P18-1177/", // link
				            // null,
				            "@Inproceedings{du2018harvesting,<br>" + // bib
				            "&nbsp;&nbsp;&nbsp;title={Harvesting Paragraph-level Question-Answer Pairs from Wikipedia},<br>" +
				            "&nbsp;&nbsp;&nbsp;author={Du, Xinya and Cardie, Claire},<br>" +
				            "&nbsp;&nbsp;&nbsp;booktitle={ACL},<br>" +
				            "&nbsp;&nbsp;&nbsp;year={2018}<br>}",
				            "We study the task of generating from Wikipedia articles question-answer pairs that cover content beyond a single sentence. We propose a neural network approach that incorporates coreference knowledge via a novel gating mechanism. As compared to models that only take into account sentence-level information (Heilman and Smith, 2010; Du et al., 2017; Zhou et al., 2017), we find that the linguistic knowledge introduced by the coreference representation aids question generation significantly, producing models that outperform the current state-of-the-art. We apply our system (composed of an answer span extraction system and the passage-level QG system) to the 10,000 top ranking Wikipedia articles and create a corpus of over one million question-answer pairs. We provide qualitative analysis for the this large-scale generated corpus from Wikipedia.", // abstract
				            // null,
				            "https://arxiv.org/pdf/1805.05942.pdf", // arXiv_link
				            "https://github.com/xinyadu/harvestingQA", // code
				        )
				        document.write("</ul>")

						// ---------------------------------- 2017 ------------------------------------------------------------------------------------------------------
				        document.write("<h5>2017</h5>")
				        document.write("<ul style=\"list-style: none;\">")
				        add_paper("<strong>Learning to Ask</strong>: Neural Question Generation for Reading Comprehension",
				            "<b>Xinya Du</b>, Junru Shao, Claire Cardie", // authors
				            "ACL 2017", // conference
				            "https://aclanthology.org/P17-1123/", // link
				            // null,
				            "@Inproceedings{du2017learning,<br>" + // bib
				            "&nbsp;&nbsp;&nbsp;title={Learning to Ask: Neural Question Generation for Reading Comprehension},<br>" +
				            "&nbsp;&nbsp;&nbsp;author={Du, Xinya  and Shao, Junru  and Cardie, Claire},<br>" +
				            "&nbsp;&nbsp;&nbsp;booktitle={ACL},<br>" +
				            "&nbsp;&nbsp;&nbsp;year={2017}<br>}",
				            "We study the task of generating from Wikipedia articles question-answer pairs that cover content beyond a single sentence. We propose a neural network approach that incorporates coreference knowledge via a novel gating mechanism. As compared to models that only take into account sentence-level information (Heilman and Smith, 2010; Du et al., 2017; Zhou et al., 2017), we find that the linguistic knowledge introduced by the coreference representation aids question generation significantly, producing models that outperform the current state-of-the-art. We apply our system (composed of an answer span extraction system and the passage-level QG system) to the 10,000 top ranking Wikipedia articles and create a corpus of over one million question-answer pairs. We provide qualitative analysis for the this large-scale generated corpus from Wikipedia.", // abstract
				            // null,
				            "https://arxiv.org/abs/1705.00106", // arXiv_link
				            "https://github.com/xinyadu/nqg", // code
				            null, //data
				            "files/acl17_dsc_poster.pdf", //slides
				            null, //talk
				            "<img src=\"imgs/tv-icon.png\" width=\"30px\" /> Featured in <a href=\"https://www.newscientist.com/article/2130205-inquisitive-bot-asks-questions-to-test-your-understanding/\"><em>New Scientist</em></a> <a><img src=\"imgs/newscientist.jpg\" width=\"40px\" /></a> <a href=\"http://www.techrepublic.com/article/how-researchers-trained-one-ai-system-to-start-asking-its-own-questions/\"><em>Tech Republic</em></a> <a><img src=\"imgs/techrepublic.png\" width=\"70px\" /></a>", //msg
				        )
				        add_paper("Identifying Where to Focus in Reading Comprehension for Neural Question Generation",
				            "<b>Xinya Du</b>, Claire Cardie", // authors
				            "EMNLP 2017", // conference
				            "https://aclanthology.org/D17-1219/", // link
				            // null,
				            "@Inproceedings{du2017identifying,<br>" + // bib
				            "&nbsp;&nbsp;&nbsp;title={Identifying Where to Focus in Reading Comprehension for Neural Question Generation},<br>" +
				            "&nbsp;&nbsp;&nbsp;author={Du, Xinya and Cardie, Claire},<br>" +
				            "&nbsp;&nbsp;&nbsp;booktitle={EMNLP},<br>" +
				            "&nbsp;&nbsp;&nbsp;year={2017}<br>}",
				            "A first step in the task of automatically generating questions for testing reading comprehension is to identify question-worthy sentences, i.e. sentences in a text passage that humans find it worthwhile to ask questions about. We propose a hierarchical neural sentence-level sequence tagging model for this task, which existing approaches to question generation have ignored. The approach is fully data-driven — with no sophisticated NLP pipelines or any hand-crafted rules/features — and compares favorably to a number of baselines when evaluated on the SQuAD data set. When incorporated into an existing neural question generation system, the resulting end-to-end system achieves state-of-the-art performance for paragraph-level question generation for reading comprehension.", // abstract
				            null,
				            // "https://arxiv.org/abs/1705.00106", // arXiv_link
				            null,
				            // "https://github.com/xinyadu/nqg", // code
				            null, //data

				            // "./files/acl17_dsc_poster.pdf", //slides
				            null, //talk
				        )
				        document.write("</ul>")

				</script>
					<!-- </script> -->
					<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
			</section>
		</div>
	</body>