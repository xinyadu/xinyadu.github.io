<!doctype html>
<html lang="en">

	<head>
		<title>Xinya Du</title>

		<!-- Required meta tags -->
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

		<!-- Bootstrap CSS -->
		<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
		<link rel="stylesheet" href="css/my.css">

		<!-- from bootstrap -->
		<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
		<script src="https://cdn.jsdelivr.net/npm/popper.js@1.14.6/dist/umd/popper.min.js" integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut" crossorigin="anonymous"></script>
		<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.2.1/dist/js/bootstrap.min.js" integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k" crossorigin="anonymous"></script>

		<!-- from CDQ -->
		<script>
			function copy(dest, source) {
				if(dest.source == source) {
				dest.innerHTML = "";
				dest.source = null;
				dest.style.width="0px";
				dest.style.border = "";
				dest.style.padding = "0px";
				}
				else {
				dest.innerHTML = source.innerHTML;
				dest.source = source;
				dest.style.width = "800px";
				dest.style.padding = "10px";
				dest.style.border = "1px solid gray";
				dest.style.background = "#FFFFFF" //"#F5F5F5";
				dest.style.margin = "10px";
			  }
			  dest.blur();
			}
		</script>	


		<!--  toggle selections -->
		<style>
			.publication-list {
				display: none;
			}
			.active-btn {
				background-color: #007bff;
				color: white;
			}
			.inactive-btn {
				background-color: #6c757d;
				color: white;
			}
			.publication-item {
				margin-bottom: 1em;
			}
			#selectedPublicationList {
				display: block;
			}

			.btn-info {
        		display: inline-block;
        		margin-right: 5px;
    		}

    		.content {
        		display: none;
        		margin-top: 5px;
    		}

			.content-box {
				border: 1px dotted #000;  /* Dotted black border */
				padding: 10px;
				margin-top: 5px;
				margin-bottom: 5px;
				display: none;
			}
			.btn-info {
				display: inline-block;
				margin-right: 5px;
				margin-bottom: 5px;
			}

		</style>

		<!-- script for pubs -->
		<script>
			function togglePublications(showSelected) {
				var selectedPublicationList = document.getElementById("selectedPublicationList");
				var allPublicationList = document.getElementById("allPublicationList");
				var selectedButton = document.getElementById('selectedButton');
				var allButton = document.getElementById('allButton');

				if (showSelected) {
					selectedPublicationList.style.display = "block";
					allPublicationList.style.display = "none";
					selectedButton.classList.add('active-btn');
					selectedButton.classList.remove('inactive-btn');
					allButton.classList.remove('active-btn');
					allButton.classList.add('inactive-btn');
				} else {
					selectedPublicationList.style.display = "none";
					allPublicationList.style.display = "block";
					selectedButton.classList.remove('active-btn');
					selectedButton.classList.add('inactive-btn');
					allButton.classList.add('active-btn');
					allButton.classList.remove('inactive-btn');
				}
			}
        	// Ensure the selected publications are shown by default when the page loads
        	window.onload = function() {
            	togglePublications(true);
        	}
		</script>
		
		<!-- add paper function-->
		<script>
			function toggleDisplay(id) {
				const element = document.getElementById(id);
				const bibElements = document.querySelectorAll('[id^="bib"]');
				const abstractElements = document.querySelectorAll('[id^="abstract"]');

				bibElements.forEach(el => {
					if (el.id !== id) {
						el.style.display = "none";
					}
				});

				abstractElements.forEach(el => {
					if (el.id !== id) {
						el.style.display = "none";
					}
				});

				element.style.display = element.style.display === "none" ? "block" : "none";
        	}

		

			function add_paper(title, authors, conference, link, bib, abstract, arxiv_link, code, data, slides, talk, msg) {
				list_entry = "<li style=\"font-size:15px\">"
				if (link != null)
					list_entry += "<a href=\"" + link + "\">"
				list_entry += "<b>" + title + "</b>"
				if (link != null)
					list_entry += "</a>"
				list_entry += "<br>" + authors + "<br>" + conference + "</li>"

				// Danqi version
				// if (bib != null) {
				// 	list_entry += "<div id=\"bib" + paper_count + "\" style=\"display:none\">" + bib + "</div>"
				// 	list_entry += "<a href=\"javascript:copy(div" + paper_count + ",bib" + paper_count + ")\"> <span class=\"btn-sm btn-info\">bib</span></a>"
				// }

				// if (abstract != null) {
				// 	list_entry += "<div id=\"abstract" + paper_count + "\" style=\"display:none\">" + abstract + "</div>"
				// 	list_entry += " <a href=\"javascript:copy(div" + paper_count + ",abstract" + paper_count + ")\"> <span class=\"btn-sm btn-info\">abstract</span></a>"
				// }


				if (bib != null) {
					list_entry += `<a href="javascript:void(0);" onclick="toggleDisplay('bib${paper_count}')" class="btn-sm btn-info">bib</a>
                            <div id="bib${paper_count}" class="content-box" style="display:none">${bib}</div>`;
            	}


            	if (abstract != null) {
                	list_entry += `<a href="javascript:void(0);" onclick="toggleDisplay('abstract${paper_count}')" class="btn-sm btn-info">abstract</a>
                            <div id="abstract${paper_count}" class="content-box" style="display:none">${abstract}</div>`;
            	}


				if (arxiv_link != null)
					list_entry += " <a target=\"_blank\" href=\"" + arxiv_link + "\" class=\"btn-sm btn-info\">arXiv</a>"

				if (code != null)
					list_entry += " <a target=\"_blank\" href=\"" + code + "\" class=\"btn-sm btn-info\">code</a>"

				if (data != null)
					list_entry += " <a target=\"_blank\" href=\"" + data + "\" class=\"btn-sm btn-info\">data</a>"

				if (slides != null)
					list_entry += " <a target=\"_blank\" href=\"" + slides + "\" class=\"btn-sm btn-info\">presentation</a>"

				if (talk != null)
					list_entry += " <a target=\"_blank\" href=\"" + talk + "\" class=\"btn-sm btn-info\">talk</a>"

				list_entry += "<br>"

				if (msg != null)
					list_entry += "<i>" + msg + "</i>"

				list_entry += "<div id=\"div" + paper_count + "\" style=\"font-size:15px\"></div><br>"

				document.write(list_entry)

				paper_count += 1
			}
		</script>

		
	</head>


 	<body>
	    <!-- Navigation -->
	    <nav class="navbar fixed-top navbar-expand-lg navbar-light" style="background-color: #cce5ff;">
	        <div class="container">
	            <!-- <a class="navbar-brand" href="#">Navbar</a> -->
	            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
	                <span class="navbar-toggler-icon"></span>
	            </button>

	            <div class="collapse navbar-collapse" id="navbarSupportedContent">

			        <ul class="navbar-nav mr-auto">
			        	<!-- <li class="nav-item active"> -->
			            	<!-- <a class="nav-link" href="#"><b>Home</b> <span class="sr-only">(current)</span></a> -->
			            <!-- </li> -->
						<li class="nav-item">
			            	<a class="nav-link" href="index.html">Home</a>
			            </li>
			            <li class="nav-item">
			            	<a class="nav-link" href="index.html#research_interest">Research Interest</a>
			            </li>
			            <li class="nav-item">
			            	<a class="nav-link" href="publications.html"><b>Publications</b></a>
			            </li>
			            <li class="nav-item">
			            	<a class="nav-link" href="advising.html">Advising</a>
			            </li>
			            <li class="nav-item">
			            	<a class="nav-link" href="teaching_service.html">Teaching &amp; Service</a>
			            </li>
			            <li class="nav-item">
			            	<a class="nav-link" href="recruiting.html">🌟Recruiting</a>
			            </li>
		          	</ul>
	            </div>  
	    	</div>
	    </nav>
	    <br><br>

 		<div class="container mt-4">
         	<!-- Publications -->
         	<hr>
         	<section name="publications" id="publications" style="scroll-margin-top: 55px">
	        	<h4>Papers <a href="https://scholar.google.com/citations?hl=en&user=R-lKQqkAAAAJ&view_op=list_works&sortby=pubdate"> (Google Scholar)</a> </h4>
				<p>(† indicates that I am a co-leading author; * indicates equal contributions.)</p>
				<button id="selectedButton" class="btn" onclick="togglePublications(true)">Show Selected Papers</button>
				<button id="allButton" class="btn" onclick="togglePublications(false)">Show All Papers</button>


				<div id="selectedPublicationList" class="publication-list mt-3">
					<script>
						paper_count = 0

						document.write("<ul style=\"list-style: none;\">")

						// 

						add_paper("LDC: Learning to Generate Research Idea with Dynamic Control",
							"Ruochen Li, Liqiang Jing, Chi Han, Jiawei Zhou, <strong>Xinya Du</strong>", // authors
							"AAAI AI4Research 2025 (Oral), ACL ARR (Under Submission)", // conference
							"https://arxiv.org/abs/2412.14626", // link
							"n/a", // bib.
							// "@misc{jing2024fgaif,<br>" + // bib
							// "&nbsp;&nbsp;&nbsp;title={FGAIF: Aligning Large Vision-Language Models with Fine-grained AI Feedback},<br>" +
							// "&nbsp;&nbsp;&nbsp;author={Liqiang Jing and Xinya Du},<br>" +
							// "&nbsp;&nbsp;&nbsp;eprint={2404.05046},<br>" +
							// "&nbsp;&nbsp;&nbsp;year={2024}<br>}",
							"The rapid advancements in large language models (LLMs) have demonstrated their potential to accelerate scientific discovery, particularly in automating the process of research ideation. LLM-based systems have shown promise in generating hypotheses and research ideas. However, current approaches predominantly rely on prompting-based pre-trained models, limiting their ability to optimize generated content effectively. Moreover, they also lack the capability to deal with the complex interdependence and inherent restrictions among novelty, feasibility, and effectiveness, which remains challenging due to the inherent trade-offs among these dimensions, such as the innovation-feasibility conflict. To address these limitations, we for the first time propose fine-tuning LLMs to be better idea proposers and introduce a novel framework that employs a two-stage approach combining Supervised Fine-Tuning (SFT) and controllable Reinforcement Learning (RL). In the SFT stage, the model learns foundational patterns from pairs of research papers and follow-up ideas. In the RL stage, multi-dimensional reward modeling, guided by fine-grained feedback, evaluates and optimizes the generated ideas across key metrics. Dimensional controllers enable dynamic adjustment of generation, while a sentencelevel decoder ensures context-aware emphasis during inference. Our framework provides a balanced approach to research ideation, achieving high-quality outcomes by dynamically navigating the trade-offs among novelty, feasibility, and effectiveness.",
							"https://arxiv.org/abs/2412.14626", // arXiv_link
							
							null, // code
							null, // data
							null, //slides
							null,
							"<a href=\"https://openreview.net/forum?id=zCb0dPvGYN\" style=\"color: red;\">Nominated for Best Paper</a>",
						)


						add_paper("DSBench: How Far Are Data Science Agents to Become Data Science Experts?",
							"Liqiang Jing, Zhehui Huang, Xiaoyang Wang, Wenlin Yao, Wenhao Yu, Kaixin Ma, Hongming Zhang, <strong>Xinya Du</strong>, Dong Yu", // authors
							"ICLR 2025", // conference
							"https://arxiv.org/abs/2409.07703", // link
							"n/a", // bib.
							// "@misc{jing2024fgaif,<br>" + // bib
							// "&nbsp;&nbsp;&nbsp;title={FGAIF: Aligning Large Vision-Language Models with Fine-grained AI Feedback},<br>" +
							// "&nbsp;&nbsp;&nbsp;author={Liqiang Jing and Xinya Du},<br>" +
							// "&nbsp;&nbsp;&nbsp;eprint={2404.05046},<br>" +
							// "&nbsp;&nbsp;&nbsp;year={2024}<br>}",
							"Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) have demonstrated impressive language/vision reasoning abilities, igniting the recent trend of building agents for targeted applications such as shopping assistants or AI software engineers. Recently, many data science benchmarks have been proposed to investigate their performance in the data science domain. However, existing data science benchmarks still fall short when compared to real-world data science applications due to their simplified settings. To bridge this gap, we introduce DSBench, a comprehensive benchmark designed to evaluate data science agents with realistic tasks. This benchmark includes 466 data analysis tasks and 74 data modeling tasks, sourced from Eloquence and Kaggle competitions. DSBench offers a realistic setting by encompassing long contexts, multimodal task backgrounds, reasoning with large data files and multi-table structures, and performing end-to-end data modeling tasks. Our evaluation of state-of-the-art LLMs, LVLMs, and agents shows that they struggle with most tasks, with the best agent solving only 34.12% of data analysis tasks and achieving a 34.74% Relative Performance Gap (RPG). These findings underscore the need for further advancements in developing more practical, intelligent, and autonomous data science agents.",
							"https://arxiv.org/abs/2409.07703", // arXiv_link
							null, // code
							"https://github.com/LiqiangJing/DSBench", // data
							null, //slides
							null,
							"<a href=\"https://huggingface.co/papers/2409.07703\" style=\"color: red;\">HuggingFace #1 Paper of the day </a>",
						)

						add_paper("FG-PRM: Fine-grained Hallucination Detection and Mitigation in Language Model Mathematical Reasoning",
							"Ruosen Li, Ziming Luo, <strong>Xinya Du</strong>", // authors
							"arXiv preprint", // conference
							"https://arxiv.org/abs/2410.06304", // link
							"n/a", // bib.
							// "@misc{jing2024fgaif,<br>" + // bib
							// "&nbsp;&nbsp;&nbsp;title={FGAIF: Aligning Large Vision-Language Models with Fine-grained AI Feedback},<br>" +
							// "&nbsp;&nbsp;&nbsp;author={Liqiang Jing and Xinya Du},<br>" +
							// "&nbsp;&nbsp;&nbsp;eprint={2404.05046},<br>" +
							// "&nbsp;&nbsp;&nbsp;year={2024}<br>}",
							"Hallucinations in large language models (LLMs) pose significant challenges in tasks requiring complex multi-step reasoning, such as mathematical problem-solving. Existing approaches primarily detect the presence of hallucinations but lack a nuanced understanding of their types and manifestations. In this paper, we first introduce a comprehensive taxonomy that categorizes the common hallucinations in mathematical reasoning task into six types: fabrication, factual inconsistency, context inconsistency, instruction inconsistency, logical inconsistency, and logical error. We then propose FG-PRM (Fine-Grained Process Reward Model), an augmented model designed to detect and mitigate hallucinations in a fine-grained, step-level manner. To address the limitations of manually labeling training data, we propose an automated method for generating fine-grained hallucination data using LLMs. By injecting hallucinations into reasoning steps of correct solutions, we create a diverse and balanced synthetic dataset for training FG-PRM, which consists of six specialized Process Reward Models (PRMs), each tailored to detect a specific hallucination type. Our FG-PRM demonstrates superior performance across two key tasks: 1) Fine-grained hallucination detection: classifying hallucination types for each reasoning step; and 2) Verification: ranking multiple LLM-generated outputs to select the most accurate solution, mitigating reasoning hallucinations. Our experiments show that FG-PRM outperforms ChatGPT-3.5 and Claude-3 on fine-grained hallucination detection and substantially boosts the performance of LLMs on GSM8K and MATH benchmarks.",
							// "https://arxiv.org/abs/2408.14033", // arXiv_link
							// "https://github.com/xinyadu/RGQA", // code
							// "abc", // data
							// "abc" // pre
						)


						add_paper("MLR-copilot: Autonomous machine learning research based on large language models agents",
							"Ruochen Li, Teerth Patel, Qingyun Wang, <strong>Xinya Du</strong>", // authors
							"arXiv preprint", // conference
							"https://arxiv.org/abs/2408.14033", // link
							"n/a", // bib.
							// "@misc{jing2024fgaif,<br>" + // bib
							// "&nbsp;&nbsp;&nbsp;title={FGAIF: Aligning Large Vision-Language Models with Fine-grained AI Feedback},<br>" +
							// "&nbsp;&nbsp;&nbsp;author={Liqiang Jing and Xinya Du},<br>" +
							// "&nbsp;&nbsp;&nbsp;eprint={2404.05046},<br>" +
							// "&nbsp;&nbsp;&nbsp;year={2024}<br>}",
							"Machine learning research, crucial for technological advancements and innovation, often faces significant challenges due to its inherent complexity, slow pace of experimentation, and the necessity for specialized expertise. Motivated by this, we present a new systematic framework, autonomous Machine Learning Research with large language models (MLR-Copilot), designed to enhance machine learning research productivity through the automatic generation and implementation of research ideas using Large Language Model (LLM) agents. The framework consists of three phases: research idea generation, experiment implementation, and implementation execution. First, existing research papers are used to generate hypotheses and experimental plans vis IdeaAgent powered by LLMs. Next, the implementation generation phase translates these plans into executables with ExperimentAgent. This phase leverages retrieved prototype code and optionally retrieves candidate models and data. Finally, the execution phase, also managed by ExperimentAgent, involves running experiments with mechanisms for human feedback and iterative debugging to enhance the likelihood of achieving executable research outcomes. We evaluate our framework on five machine learning research tasks and the experimental results show the framework's potential to facilitate the research progress and innovations.",
							"https://arxiv.org/abs/2408.14033", // arXiv_link
							"https://github.com/du-nlp-lab/MLR-Copilot", // code
							// "abc", // data
							// "abc" // pre
						)

						add_paper("FGAIF: Aligning Large Vision-Language Models with Fine-grained AI Feedback",
							"Liqiang Jing, <strong>Xinya Du</strong>", // authors
							"arXiv preprint arXiv:2404.05046", // conference
							"https://arxiv.org/abs/2404.05046", // link
							// "n/a", // bib.
							"@misc{jing2024fgaif,<br>" + // bib
							"&nbsp;&nbsp;&nbsp;title={FGAIF: Aligning Large Vision-Language Models with Fine-grained AI Feedback},<br>" +
							"&nbsp;&nbsp;&nbsp;author={Liqiang Jing and Xinya Du},<br>" +
							"&nbsp;&nbsp;&nbsp;eprint={2404.05046},<br>" +
							"&nbsp;&nbsp;&nbsp;year={2024}<br>}",
							"Large Vision-Language Models (LVLMs) have demonstrated proficiency in tackling a variety of visual-language tasks. However, current LVLMs suffer from misalignment between text and image modalities which causes three kinds of hallucination problems, i.e., object existence, object attribute, and object relationship. To tackle this issue, existing methods mainly utilize Reinforcement Learning (RL) to align modalities in LVLMs. However, they still suffer from three main limitations: (1) General feedback can not indicate the hallucination type contained in the response; (2) Sparse rewards only give the sequence-level reward for the whole response; and (3)Annotation cost is time-consuming and labor-intensive. To handle these limitations, we propose an innovative method to align modalities in LVLMs through Fine-Grained Artificial Intelligence Feedback (FGAIF), which mainly consists of three steps: AI-based Feedback Collection, Fine-grained Reward Model Training, and Reinforcement Learning with Fine-grained Reward. Specifically, We first utilize AI tools to predict the types of hallucination for each segment in the response and obtain a collection of fine-grained feedback. Then, based on the collected reward data, three specialized reward models are trained to produce dense rewards. Finally, a novel fine-grained feedback module is integrated into the Proximal Policy Optimization (PPO) algorithm. Extensive experiments are conducted on hallucination and general benchmarks, demonstrating the superior performance of our proposed method. Notably, compared with previous models trained with the RL-based aligning method, our proposed method is effective even with fewer parameters.",
							"https://arxiv.org/abs/2404.05046", // arXiv_link
							// "https://github.com/xinyadu/RGQA", // code
							// "abc", // data
							// "abc" // pre
						)


						add_paper("IQA-EVAL: Automatic Evaluation of Human-Model Interactive Question Answering", 
						"Ruosen Li, Ruochen Li, Barry Wang, <strong>Xinya Du</strong>", 
						"In NeurIPS, 2024", 
						"https://arxiv.org/abs/2311.01477", // link
						"", 
						"To evaluate Large Language Models (LLMs) for question answering (QA), traditional methods typically focus on directly assessing the immediate responses generated by the models based on the given question and context. In the common use case of humans seeking AI assistant’s help in finding information, these noninteractive evaluations do not account for the dynamic nature of human-model conversations, and interaction-aware evaluations have shown that accurate models are not necessarily preferred by humans [22]. Recent works in human-computer interaction (HCI) have employed human evaluators to conduct interactions and evaluations, but they are often prohibitively expensive and time-consuming to scale. In this work, we introduce an automated evaluation framework IQA-E VAL to Interactive Question Answering Evaluations, more specifically, we introduce LLM-based Evaluation Agent (LEA) that can: (1) simulate human behaviors to generate interactions with IQA models; (2) automatically evaluate the generated interactions. Moreover, we propose assigning personas to LEAs to better simulate groups of real human evaluators. We show that: (1) our evaluation framework with GPT-4 (or Claude) as the backbone model achieves a high correlation with human evaluations on the IQA task; (2) assigning personas to LEA to better represent the crowd further significantly improves correlations. Finally, we use our automated metric to evaluate five recent representative LLMs with over 1000 questions from complex and ambiguous question answering tasks, which comes with a substantial cost of $5k if evaluated by humans.", 
						"");

						add_paper("FaithScore: Fine-grained Evaluations of Hallucinations in Large Vision-Language Models", 
						"Liqiang Jing, Ruosen Li, Yunmo Chen, <strong>Xinya Du</strong>", 
						"In EMNLP (Findings), 2024", 
						"https://arxiv.org/abs/2311.01477", // link
						// "n/a", // bib.
						"@misc{jing2023faithscore,<br>" + // bib
						"&nbsp;&nbsp;&nbsp;title={FAITHSCORE: Evaluating Hallucinations in Large Vision-Language Models},<br>" +
						"&nbsp;&nbsp;&nbsp;author={Liqiang Jing and Ruosen Li and Yunmo Chen and Mengzhao Jia and Xinya Du},<br>" +
						"&nbsp;&nbsp;&nbsp;eprint={2311.01477},<br>" +
						"&nbsp;&nbsp;&nbsp;year={2023}<br>}",
						"We introduce FAITHSCORE (Faithfulness to Atomic Image Facts Score), a reference-free and fine-grained evaluation metric that measures the faithfulness of the generated free-form answers from large vision-language models (LVLMs). The FAITHSCORE evaluation first identifies sub-sentences containing descriptive statements that need to be verified, then extracts a comprehensive list of atomic facts from these sub-sentences, and finally conducts consistency verification between fine-grained atomic facts and the input image. Meta-evaluation demonstrates that our metric highly correlates with human judgments of faithfulness. We collect two benchmark datasets (i.e. LLaVA-1k and MSCOCO-Cap) for evaluating LVLMs instruction-following hallucinations. We measure hallucinations in state-of-the-art LVLMs with FAITHSCORE on the datasets. Results reveal that current systems are prone to generate hallucinated content unfaithful to the image, which leaves room for future improvements. Further, we find that current LVLMs despite doing well on color and counting, still struggle with long answers, relations, and multiple objects.", // abstract
						"https://arxiv.org/abs/2311.01477", // arXiv_link
						// "https://github.com/xinyadu/RGQA", // code
						// "abc", // data
						// "abc" // pre
						);

						// add_paper("A Benchmark for Multi-hop Event-centric Question Answering with Explanations", 
						// "Ruosen Li, Zimu Wang, Son Quoc Tran, Lei Xia, <strong>Xinya Du</strong>", 
						// "In NeurIPS, 2024", 
						// "null", 
						// "", 
						// "", 
						// "", 
						// "");


						add_paper("Large Language Models for Automated Open-domain Scientific Hypotheses Discovery",
						"Zonglin Yang, <strong>Xinya Du</strong> †, Junxian Li, Jie Zheng, Soujanya Poria, Erik Cambria", // authors
						"ACL 2024 (Findings)", // conference
						"https://arxiv.org/abs/2309.02726", // link
						"@article{yang2024large,<br>" + // bib
						"&nbsp;&nbsp;&nbsp;title={Large Language Models for Automated Open-domain Scientific Hypotheses Discovery},<br>" +
						"&nbsp;&nbsp;&nbsp;author={Zonglin Yang and Xinya Du and Junxian Li and Jie Zheng and Soujanya Poria and Erik Cambria},<br>" +
						"&nbsp;&nbsp;&nbsp;booktitle={Findings of the Association for Computational Linguistics: ACL 2024},<br>" +
						"&nbsp;&nbsp;&nbsp;year={2024}<br>}",
						"Hypothetical induction is recognized as the main reasoning type when scientists make observations about the world and try to propose hypotheses to explain those observations. Past research on hypothetical induction is under a constrained setting: (1) the observation annotations in the dataset are carefully manually handpicked sentences (resulting in a close-domain setting); and (2) the ground truth hypotheses are mostly commonsense knowledge, making the task less challenging. In this work, we tackle these problems by proposing the first NLP dataset for social science academic hypotheses discovery, consisting of 50 recent top social science publications; and a raw web corpus that contains enough information to make it possible to develop all the research hypotheses in the 50 papers. The final goal is to create systems that automatically generate valid, novel, and helpful scientific hypotheses, given only a pile of raw web corpus. Different from the previous settings, the new dataset requires (1) using open-domain data (raw web corpus) as observations; and (2) proposing hypotheses even new to humanity. A multi-module framework is developed for the task, as well as three different feedback mechanisms that empirically show performance gain over the base framework. Finally, our framework exhibits superior performance in terms of both GPT-4 based evaluation and expert-based this http URL the best of our knowledge, this is the first work showing that LLMs are able to generate novel (not existing in the literature) and valid (reflecting reality) scientific hypotheses.", // abstract
						"https://arxiv.org/abs/2309.02726", // arXiv_link
							"", // code
							null, //data
							"files/acl17_dsc_poster.pdf", //slides
							null, //talk
							"<a href=\"https://ai4sciencecommunity.github.io/icml24/award.html\" style=\"color: red;\"><strong>🏆 Best poster award at ICML AI for Science (1\% of 250 submissions)</strong></a>",
						)


						add_paper("PRD: Peer Rank and Discussion Improve Large Language Model based Evaluations",
						"Ruosen Li, Teerth Patel, <strong>Xinya Du</strong>", // authors
						"TMLR 2024", // conference
						"https://arxiv.org/pdf/2307.02762.pdf", // link
						"@article{li2023prd,<br>" + // bib
						"&nbsp;&nbsp;&nbsp;title={PRD: Peer Rank and Discussion Improve Large Language Model based Evaluations},<br>" +
						"&nbsp;&nbsp;&nbsp;author={Li, Ruosen and Patel, Teerth and Du, Xinya},<br>" +
						"&nbsp;&nbsp;&nbsp;booktitle={TMLR},<br>" +
						"&nbsp;&nbsp;&nbsp;year={2024}<br>}",
						"Nowadays, the quality of responses generated by different modern large language models (LLMs) are hard to evaluate and compare automatically. Recent studies suggest and predominantly use LLMs as a reference-free metric for open-ended question answering. More specifically, they use the recognized strongest LLM as the evaluator, which conducts pairwise comparisons of candidate models' answers and provides a ranking score. However, this intuitive method has multiple problems, such as bringing in self-enhancement (favoring its own answers) and positional bias. We draw insights and lessons from the educational domain (Cho and MacArthur, 2011; Walsh, 2014) to improve LLM-based evaluations. Specifically, we propose the (1) peer rank (PR) algorithm that takes into account each peer LLM's pairwise preferences of all answer pairs, and outputs a final ranking of models; and (2) peer discussion (PD), where we prompt two LLMs to discuss and try to reach a mutual agreement on preferences of two answers. We conduct experiments on two benchmark datasets. We find that our approaches achieve higher accuracy and align better with human judgments, respectively. Interestingly, PR can induce a relatively accurate self-ranking of models under the anonymous setting, where each model's name is unrevealed. Our work provides space to explore evaluating models that are hard to compare for humans.", // abstract
						"https://arxiv.org/abs/2307.02762", // arXiv_link
						// "https://github.com/xinyadu/RGQA", // code
						// "abc", // data
						// "abc" // pre
						)

						add_paper("Document-level Causal Relation Extraction with Knowledge-guided Binary Question Answering", 
						"Zimu Wang, Lei Xia, Wei Wang, <strong>Xinya Du</strong>", 
						"In EMNLP (Findings), 2024", 
						"", 
						"", 
						"", 
						"", 
						"");

					

						add_paper("Language Models as Inductive Reasoners",
						"Zonglin Yang, Li Dong, <strong>Xinya Du</strong> †, Hao Cheng, Erik Cambria, Xiaodong Liu, Jianfeng Gao, Furu Wei", // authors
						"EACL 2024", // conference
						"https://arxiv.org/pdf/2212.10923.pdf", // link
						"@article{yang2024inductive,<br>" + // bib
						"&nbsp;&nbsp;&nbsp;title={Language Models as Inductive Reasoners,<br>" +
						"&nbsp;&nbsp;&nbsp;author={Yang, Zonglin and Dong, Li and Du, Xinya and Cheng, Hao and Cambria, Erik and Liu, Xiaodong and Gao, Jianfeng and Wei, Furu},<br>" +
						"&nbsp;&nbsp;&nbsp;booktitle={EACL},<br>" +
						"&nbsp;&nbsp;&nbsp;year={2024}<br>}",
						"Inductive reasoning is a core component of human intelligence. In the past research of inductive reasoning within computer science, logic language is used as representations of knowledge (facts and rules, more speciﬁcally). However, logic language can cause systematic problems for inductive reasoning such as disability of handling raw input such as natural language, sensitiveness to mislabeled data, and incapacity to handle ambiguous input. To this end, we propose a new task, which is to induce natural language rules from natural language facts, and create a dataset termed DEER containing 1.2k rule-fact pairs for the task, where rules and facts are written in natural language. New automatic metrics are also proposed and analysed for the evaluation of this task. With DEER, we investigate a modern approach for inductive reasoning where we use natural language as representation for knowledge instead of logic language and use pretrained language models as “reasoners”. Moreover, we provide the ﬁrst and comprehensive analysis of how well pretrained language models can induce natural language rules from natural language facts. We also propose a new framework drawing insights from philosophy literature for this task, which we show in the experiment section that surpasses baselines in both automatic and human evaluations.", // abstract
						"https://arxiv.org/abs/2212.10923", // arXiv_link
						// "https://github.com/xinyadu/RGQA", // code
						// "abc", // data
						// "abc" // pre
						)

						
						add_paper("QAEvent: Event Extraction as Question-Answer Pairs Generation",
						"Milind Choudhary, <strong>Xinya Du</strong>", // authors
						"EACL 2024 (Findings)", // conference
						"https://aclanthology.org/2024.findings-eacl.126/", // link
						"@article{Choudhary2024qaevent,<br>" + // bib
						"&nbsp;&nbsp;&nbsp;title={Language Models as Inductive Reasoners},<br>" +
						"&nbsp;&nbsp;&nbsp;author={Choudhary, Milind  and Du, Xinya},<br>" +
						"&nbsp;&nbsp;&nbsp;booktitle={Findings of the Association for Computational Linguistics: EACL 2024},<br>" +
						"&nbsp;&nbsp;&nbsp;year={2024}<br>}",
						"We propose a novel representation of document-level events as question and answer pairs (QAEVENT). Under this paradigm: (1) questions themselves can define argument roles without the need for predefined schemas, which will cover a comprehensive list of event arguments from the document; (2) it allows for more scalable and faster annotations from crowdworkers without linguistic expertise. Based on our new paradigm, we collect a novel and wide-coverage dataset. Our examinations show that annotations with the QA representations produce high-quality data for document-level event extraction, both in terms of human agreement level and high coverage of roles comparing to the pre-defined schema. We present and compare representative approaches for generating event question answer pairs on our benchmark.", // abstract
						null, // arXiv_link
						"https://github.com/Milind21/qag_ee", // code
						// "abc", // data
						// "abc" // pre
						)





						add_paper("Proto-CLIP: Vision-Language Prototypical Network for Few-Shot Learning",
						"Jishnu Jaykumar P, Kamalesh Palanisamy, Yu-Wei Chao, <strong>Xinya Du</strong>, Yu Xiang", // authors
						"IROS 2024.", // conference
						"https://arxiv.org/pdf/2307.03073", // link
						"@article{yang2024inductive,<br>" + // bib
						"&nbsp;&nbsp;&nbsp;title={Language Models as Inductive Reasoners,<br>" +
						"&nbsp;&nbsp;&nbsp;author={Yang, Zonglin and Dong, Li and Du, Xinya and Cheng, Hao and Cambria, Erik and Liu, Xiaodong and Gao, Jianfeng and Wei, Furu},<br>" +
						"&nbsp;&nbsp;&nbsp;booktitle={EACL},<br>" +
						"&nbsp;&nbsp;&nbsp;year={2024}<br>}",
						"...", // abstract
						"https://arxiv.org/pdf/2307.03073", // arXiv_link
						// "https://github.com/xinyadu/RGQA", // code
						// "abc", // data
						// "abc" // pre
						)


						add_paper("Leveraging Structured Information for Explainable Multi-hop Question Answering and Reasoning",
						"Ruosen Li, <strong>Xinya Du</strong>", // authors
						"EMNLP 2023 (Findings)", // conference
						"https://aclanthology.org/2023.findings-emnlp.452/", // link
						"@InProceedings{li2023leveraging,<br>" + // bib
						"&nbsp;&nbsp;&nbsp;title={Leveraging Structured Information for Explainable Multi-hop Question Answering and Reasoning},<br>" +
						"&nbsp;&nbsp;&nbsp;author={Li, Ruosen and Du, Xinya},<br>" +
						"&nbsp;&nbsp;&nbsp;booktitle={EMNLP},<br>" +
						"&nbsp;&nbsp;&nbsp;year={2023}<br>}",
						"Neural models, including large language models (LLMs), achieve superior performance on multi-hop question-answering. To elicit reasoning capabilities from LLMs, recent works propose using the chain-of-thought (CoT) mechanism to generate both the reasoning chain and the answer, which enhances the model’s capabilities in conducting multi-hop reasoning. However, several challenges still remain: such as struggling with inaccurate reasoning, hallucinations, and lack of interpretability. On the other hand, information extraction (IE) identifies entities, relations, and events grounded to the text. The extracted structured information can be easily interpreted by humans and machines (Grishman, 2019). In this work, we investigate constructing and leveraging extracted semantic structures (graphs) for multihop question answering, especially the reasoning process. Empirical results and human evaluations show that our framework: generates more faithful reasoning chains and substantially improves the QA performance on two benchmark datasets. Moreover, the extracted structures themselves naturally provide grounded explanations that are preferred by humans, as compared to the generated reasoning chains and saliency-based explanations.", // abstract
						"n/a", // arXiv_link
						// "https://github.com/xinyadu/RGQA", // code
						// "abc", // data
						// "abc" // pre
						)

						add_paper("POE: Process of Elimination for Multiple Choice Reasoning",
						"Chenkai Ma, <strong>Xinya Du</strong>", // authors
						"EMNLP 2023", // conference
						"https://aclanthology.org/2023.emnlp-main.273.pdf", // link
						"@InProceedings{li2023leveraging,<br>" + // bib
						"&nbsp;&nbsp;&nbsp;title={Process of Elimination for Multiple Choice Reasoning},<br>" +
						"&nbsp;&nbsp;&nbsp;author={Ma, Chenkai and Du, Xinya},<br>" +
						"&nbsp;&nbsp;&nbsp;booktitle={EMNLP},<br>" +
						"&nbsp;&nbsp;&nbsp;year={2023}<br>}",
						"Language models are capable of in-context learning for multiple choice reasoning tasks, but the options in these tasks are treated equally. As human often eliminate wrong options before reaching the final answer, we argue a similar strategy can make LMs better at these tasks. To this end, we present Process of Elimination (PoE), a two-step prompting method. In the first step, PoE scores each option, and eliminates seemingly wrong options. In the second step, PoE masks wrong options, and makes the final prediction from the remaining options. Comprehensive experiments on 8 reasoning benchmarks illustrate the effect of our method. Analysis shows our method is applicable to fewshot and compatible with LLMs like ChatGPT.", // abstract
						"n/a", // arXiv_link
						// "https://github.com/xinyadu/RGQA", // code
						// "abc", // data
						// "abc" // pre
						)

						add_paper("Probing Representations for Document-level Event Extraction",
						"Barry Wang, <strong>Xinya Du</strong>, Claire Cardie", // authors
						"EMNLP 2023 (Findings)", // conference
						"https://aclanthology.org/2023.findings-emnlp.844/", // link
						"@article{wang2023probing,<br>" + // bib
						"&nbsp;&nbsp;&nbsp;title={Probing Representations for Document-level Event Extraction},<br>" +
						"&nbsp;&nbsp;&nbsp;author={Wang, Barry and Du, Xinya and Cardie, Claire},<br>" +
						"&nbsp;&nbsp;&nbsp;booktitle={EMNLP},<br>" +
						"&nbsp;&nbsp;&nbsp;year={2023}<br>}",
						"The probing classifiers framework has been employed for interpreting deep neural network models for a variety of natural language processing (NLP) applications. Studies, however, have largely focused on sentencelevel NLP tasks. This work is the first to apply the probing paradigm to representations learned for document-level information extraction (IE). We designed eight embedding probes to analyze surface, semantic, and event-understanding capabilities relevant to document-level event extraction. We apply them to the representations acquired by learning models from three different LLM-based document-level IE approaches on a standard dataset. We found that trained encoders from these models yield embeddings that can modestly improve argument detections and labeling but only slightly enhance event-level tasks, albeit trade-offs in information helpful for coherence and event-type prediction. We further found that encoder models struggle with document length and cross-sentence discourse.", // abstract
						"n/a", // arXiv_link
						// "https://github.com/xinyadu/RGQA", // code
						// "abc", // data
						// "abc" // pre
						)

						// add_paper("Toward Consistent and Informative Event-Event Temporal Relation Extraction",
						// "Xiaomeng Jin, Haoyang Wen, <strong>Xinya Du</strong>, Heng Ji", // authors
						// "MATCHING at ACL 2023", // conference
						// "https://aclanthology.org/2023.matching-1.3.pdf", // link
						// "@InProceedings{jin2023consistent,<br>" + // bib
						// "&nbsp;&nbsp;&nbsp;title={Toward Consistent and Informative Event-Event Temporal Relation Extraction},<br>" +
						// "&nbsp;&nbsp;&nbsp;author={Jin, Xiaomeng and Wen, Haoyang and Du, Xinya and Ji, Heng},<br>" +
						// "&nbsp;&nbsp;&nbsp;booktitle={MATCHING at ACL 2023},<br>" +
						// "&nbsp;&nbsp;&nbsp;year={2023}<br>}",
						// "Event-event temporal relation extraction aims to extract the temporal order between a pair of event mentions, which is usually used to con- struct temporal event graphs. However, event graphs generated by existing methods are usu- ally globally inconsistent (event graphs contain- ing cycles), semantically irrelevant (two unre- lated events having temporal links), and context unaware (neglecting neighborhood information of an event node). In this paper, we propose a novel event-event temporal relation extrac- tion method to address these limitations. Our model combines a pretrained language model and a graph neural network to output event em- beddings, which captures the contextual infor- mation of event graphs. Moreover, to achieve global consistency and semantic relevance, (1) event temporal order should be in accordance with the norm of their embeddings, and (2) two events have temporal relation only if their embeddings are close enough. Experimental results on a real-world event dataset demon- strate that our method achieves state-of-the-art performance and generates high-quality event graphs.", // abstract
						// null, // arXiv_link
						// // "https://github.com/xinyadu/RGQA", // code
						// // "abc", // data
						// // "abc" // pre
						// )

						add_paper("Zero-Shot Classification by Logical Reasoning on Natural Language Explanations",
						"Chi Han, Hengzhi Pei, <strong>Xinya Du</strong>, Heng Ji", // authors
						"ACL 2023 (Findings)", // conference
						"https://aclanthology.org/2023.findings-acl.571.pdf", // link
						"@InProceedings{han2023zero,<br>" + // bib
						"&nbsp;&nbsp;&nbsp;title={Zero-Shot Classification by Logical Reasoning on Natural Language Explanations},<br>" +
						"&nbsp;&nbsp;&nbsp;author={Han, Chi and Pei, Hengzhi and Du, Xinya and Ji, Heng},<br>" +
						"&nbsp;&nbsp;&nbsp;booktitle={ACL (Findings)},<br>" +
						"&nbsp;&nbsp;&nbsp;year={2023}<br>}",
						"Humans can classify data of an unseen cate- gory by reasoning on its language explanations. This ability is owing to the compositional na- ture of language: we can combine previously seen attributes to describe the new category. For example, we might describe a sage thrasher as it has a slim straight relatively short bill, yellow eyes and a long tail, so that others can use their knowledge of attributes “slim straight relatively short bill”, “yellow eyes” and “long tail” to recognize a sage thrasher. Inspired by this observation, in this work we tackle zero-shot classification task by logically pars- ing and reasoning on natural language expla- nations. To this end, we propose the frame- work CLORE (Classification by LOgical Rea- soning on Explanations). While previous meth- ods usually regard textual information as im- plicit features, CLORE parses explanations into logical structures and then explicitly reasons along thess structures on the input to produce a classification score. Experimental results on explanation-based zero-shot classification benchmarks demonstrate that CLORE is supe- rior to baselines, which we further show mainly comes from higher scores on tasks requiring more logical reasoning. We also demonstrate that our framework can be extended to zero- shot classification on visual modality. Along- side classification decisions, CLORE can pro- vide the logical parsing and reasoning process as a clear form of rationale. Through empirical analysis we demonstrate that CLORE is also less affected by linguistic biases than baselines.", // abstract
						// "n/a", // arXiv_link
						// "https://github.com/xinyadu/RGQA", // code
						// "abc", // data
						// "abc" // pre
						)

						add_paper("End-to-end Case-Based Reasoning for Commonsense Knowledge Base Completion",
							"Zonglin Yang, <strong>Xinya Du</strong>, Erik Cambria, Claire Cardie", // authors
							"EACL 2023", // conference
							"https://aclanthology.org/2023.eacl-main.255/", // link
							"@InProceedings{yang2023cbr,<br>" + // bib
							"&nbsp;&nbsp;&nbsp;title={End-to-end Case-Based Reasoning for Commonsense Knowledge Base Completion},<br>" +
							"&nbsp;&nbsp;&nbsp;author={Yang, Zonglin and Du, Xinya and Cambria, Erik and Cardie, Claire},<br>" +
							"&nbsp;&nbsp;&nbsp;booktitle={EACL},<br>" +
							"&nbsp;&nbsp;&nbsp;year={2023}<br>}",
							"Pretrained language models have been shown to store knowledge in their parameters and have achieved reasonable performance in commonsense knowledge base completion (CKBC) tasks. However, CKBC is knowledge-intensive and it is reported that pretrained language models’ performance in knowledge-intensive tasks are limited because of their incapability of accessing and manipulating knowledge. As a result, we hypothesize that providing retrieved passages that contain relevant knowledge as additional input to the CKBC task will improve performance. In particular, we draw insights from Case-Based Reasoning (CBR) – which aims to solve a new problem by reasoning with retrieved relevant cases, and investigate the direct application of it to CKBC. On two benchmark datasets, we demonstrate through automatic and human evaluations that our End-to-end Case-Based Reasoning Framework (ECBRF) generates more valid, informative, and novel knowledge than the state-of-the-art COMET model for CKBC in both the fully supervised and few-shot settings. We provide insights on why previous retrieval-based methods only achieve merely the same performance with COMET. From the perspective of CBR, our framework addresses a fundamental question on whether CBR methodology can be utilized to improve deep learning models.", // abs
							null, // arXiv_link
							// "https://github.com/xinyadu/RGQA", // code
							// "abc", // data
							// "abc" // pre
						)

						add_paper("Logical Entity Representation in Knowledge-Graphs for Differentiable Rule Learning",
							"Chi Han, Qizheng He, Charles Yu, <b>Xinya Du</b>, Hanghang Tong, Heng Ji", // authors
							"ICLR 2023", // conference
							"https://arxiv.org/pdf/2211.07067.pdf", // link
							"@InProceedings{han2023logical,<br>" + // bib
							"&nbsp;&nbsp;&nbsp;title={Logical Entity Representation in Knowledge-Graphs for Differentiable Rule Learning},<br>" +
							"&nbsp;&nbsp;&nbsp;author={Han, Chi and He, Qizheng and Yu, Charles and Du, Xinya and Tong, Hanghang and Ji, Heng},<br>" +
							"&nbsp;&nbsp;&nbsp;booktitle={ICLR},<br>" +
							"&nbsp;&nbsp;&nbsp;year={2023}<br>}",
							"Probabilistic logical rule learning has shown great strength in logical rule mining and knowledge graph completion. It learns logical rules to predict missing edges by reasoning on existing edges in the knowledge graph. However, previous efforts have largely been limited to only modeling chain-like Horn clauses such as R1(x; z) ^ R2(z; y) ) H(x; y). This formulation overlooks additional contextual information from neighboring sub-graphs of entity variables x, y and z. Intuitively, there is a large gap here, as local sub-graphs have been found to provide important information for knowledge graph completion. Inspired by these observations, we propose Logical Entity RePresentation (LERP) to encode contextual information of entities in the knowledge graph. A LERP is designed as a vector of probabilistic logical functions on the entity’s neighboring sub-graph. It is an interpretable representation while allowing for differentiable optimization. We can then incorporate LERP into probabilistic logical rule learning to learn more expressive rules. Empirical results demonstrate that with LERP, our model outperforms other rule learning methods in knowledge graph completion and is comparable or even superior to state-of-the-art black-box methods. Moreover, we find that our model can discover a more expressive family of logical rules. LERP can also be further combined with embedding learning methods like TransE to make it more interpretable.", // abstract
							"https://openreview.net/forum?id=JdgO-ht1uTN", // arXiv_link
							// "https://github.com/xinyadu/RGQA", // code
							// "abc", // data
							// "abc" // pre
						)

						add_paper("RESIN-11: Schema-guided Event Prediction for 11 Newsworthy Scenarios",
							"<b>Xinya Du</b>, Zixuan Zhang, Sha Li, Ziqi Wang, Pengfei Yu, Hongwei Wang, Tuan Manh Lai, Xudong Lin, Iris Liu, Ben Zhou, Haoyang Wen, Manling Li, Darryl Hannan, Jie Lei, Hyounghun Kim, Rotem Dror, Haoyu Wang, Michael Regan, Qi Zeng, Qing Lyu, Charles Yu, Carl Edwards, Xiaomeng Jin, Yizhu Jiao, Ghazaleh Kazeminejad, Zhenhailong Wang, Chris Callison-Burch, Carl Vondrick, Mohit Bansal, Dan Roth, Jiawei Han, Shih-Fu Chang, Martha Palmer, Heng Ji", // authors
							"NAACL 2022 (demo)", // conference
							"https://blender.cs.illinois.edu/paper/resin2022.pdf", // link
							null, //bib
							null, //abs
							null, //arxiv
							"https://github.com/RESIN-KAIROS/RESIN-11",//code
							null, //data
							null,
							null,
							"<a href=\"https://www.darpa.mil/program/knowledge-directed-artificial-intelligence-reasoning-over-schemas\" style=\"color: red;\"><strong>🏆 Top ranking system at DARPA KAIROS Evaluation.</strong></a>"
)

						add_paper("Retrieval-Augmented Generative Question Answering for Event Argument Extraction",
							"<b>Xinya Du</b> and Heng Ji", // authors
							"Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP)", // conference
							"https://arxiv.org/pdf/2211.07067.pdf", // link
							"@InProceedings{du2022retrieval,<br>" + // bib
							"&nbsp;&nbsp;&nbsp;title={Retrieval-Augmented Generative Question Answering for Event Argument Extraction},<br>" +
							"&nbsp;&nbsp;&nbsp;author={Du, Xinya and Ji, Heng},<br>" +
							"&nbsp;&nbsp;&nbsp;booktitle={EMNLP},<br>" +
							"&nbsp;&nbsp;&nbsp;year={2022}<br>}",
							"Event argument extraction has long been studied as a sequential prediction problem with extractive-based methods, tackling each argument in isolation. Although recent work proposes generation-based methods to capture cross-argument dependency, they require generating and post-processing a complicated target sequence (template). Motivated by these observations and recent pretrained language models’ capabilities of learning from demonstrations. We propose a retrieval-augmented generative QA model (R-GQA) for event argument extraction. It retrieves the most similar QA pair and augments it as prompt to the current example’s context, then decodes the arguments as answers. Our approach outperforms substantially prior methods across various settings (i.e. fully supervised, domain transfer, and fewshot learning). Finally, we propose a clusteringbased sampling strategy (JointEnc) and conduct a thorough analysis of how different strategies influence the few-shot learning performance.", // abstract
							"https://arxiv.org/abs/2211.07067", // arXiv_link
							"https://github.com/xinyadu/RGQA", // code
							// "abc", // data
							// "abc" // pre
						)
						
						add_paper("Dynamic Global Memory for Document-level Argument Extraction",
							"<b>Xinya Du</b>, Sha Li, Heng Ji", // authors
							"Association for Computational Linguistics and the International Joint Conference on Natural Language Processing (ACL-IJCNLP) 2022", // conference
							"https://aclanthology.org/2022.acl-long.361/", // link
							"@InProceedings{du2022dynamic,<br>" + // bib
							"&nbsp;&nbsp;&nbsp;title={Dynamic Global Memory for Document-level Argument Extraction},<br>" +
							"&nbsp;&nbsp;&nbsp;author={Du, Xinya and Li, Sha and Ji, Heng},<br>" +
							"&nbsp;&nbsp;&nbsp;booktitle={Association for Computational Linguistics (ACL)},<br>" +
							"&nbsp;&nbsp;&nbsp;year={2022}<br>}",
							"Extracting informative arguments of events from news articles is a challenging problem in information extraction, which requires a global understanding of each document. While recent work on document-level extraction has gone beyond single-sentence and increased the cross-sentence inference capability of end-to-end models, they are still restricted by certain input sequence length constraints and usually ignore the global context between events. To tackle this issue, we introduce a new global neural generation-based framework for document-level event argument extraction by constructing a document memory store to record the contextual event information and leveraging it to implicitly and explicitly help with decoding of arguments for later events. Empirical results show that our framework outperforms prior methods substantially and it is more robust to adversarially annotated examples with our constrained decoding design.", // abstract
							null, // arXiv_link
							"https://github.com/xinyadu/memory_docie", // code
							// "abc", // data
							// "abc" // pre
						)

						add_paper("Template Filling with Generative Transformers",
							"<b>Xinya Du</b>, Alexander M. Rush, Claire Cardie", // authors
							"NAACL 2021 (short)", // conference
							"https://aclanthology.org/2021.naacl-main.70", // link
							null,
							// "@InProceedings{du2022dynamic,<br>" + // bib
							// "&nbsp;&nbsp;&nbsp;title={Dynamic Global Memory for Document-level Argument Extraction},<br>" +
							// "&nbsp;&nbsp;&nbsp;author={Du, Xinya and Li, Sha and Ji, Heng},<br>" +
							// "&nbsp;&nbsp;&nbsp;booktitle={Association for Computational Linguistics (ACL)},<br>" +
							// "&nbsp;&nbsp;&nbsp;year={2022}<br>}",
							"Template filling is generally tackled by a pipeline of two separate supervised systems – one for role-filler extraction and another for template/event recognition. Since pipelines consider events in isolation, they can suffer from error propagation. We introduce a framework based on end-to-end generative transformers for this task (i.e., GTT). It naturally models the dependence between entities both within a single event and across the multiple events described in a document. Experiments demonstrate that this framework substantially outperforms pipeline-based approaches, and other neural end-to-end baselines that do not model between-event dependencies. We further show that our framework specifically improves performance on documents containing multiple events.", // abstract
							null, // arXiv_link
							"https://github.com/xinyadu/gtt", // code
						)
						
						add_paper("GRIT: Generative Role-filler Transformers for <strong>Document-level</strong> Event Entity Extraction",
							"<b>Xinya Du</b>, Alexander M. Rush, Claire Cardie", // authors
							"EACL 2021", // conference
							"https://aclanthology.org/2021.eacl-main.52", // link
							null,
							// "@InProceedings{du2022dynamic,<br>" + // bib
							// "&nbsp;&nbsp;&nbsp;title={Dynamic Global Memory for Document-level Argument Extraction},<br>" +
							// "&nbsp;&nbsp;&nbsp;author={Du, Xinya and Li, Sha and Ji, Heng},<br>" +
							// "&nbsp;&nbsp;&nbsp;booktitle={Association for Computational Linguistics (ACL)},<br>" +
							// "&nbsp;&nbsp;&nbsp;year={2022}<br>}",
							"We revisit the classic problem of document-level role-filler entity extraction (REE) for template filling. We argue that sentence-level approaches are ill-suited to the task and introduce a generative transformer-based encoder-decoder framework (GRIT) that is designed to model context at the document level: it can make extraction decisions across sentence boundaries; is implicitly aware of noun phrase coreference structure, and has the capacity to respect cross-role dependencies in the template structure. We evaluate our approach on the MUC-4 dataset, and show that our model performs substantially better than prior work. We also show that our modeling choices contribute to model performance, e.g., by implicitly capturing linguistic knowledge such as recognizing coreferent entity mentions.", // abstract
							null, // arXiv_link
							"https://github.com/xinyadu/grit_doc_event_entity", // code
						)
						
						add_paper("Few-shot Intent Classification and Slot Filling with Retrieved Examples",
							"Dian Yu, Luheng He, Yuan Zhang, <strong>Xinya Du</strong>, Panupong Pasupat, Qi Li", // authors
							"NAACL 2021", // conference
							"https://aclanthology.org/2021.naacl-main.59/", // link
							null,
							// "@InProceedings{du2022dynamic,<br>" + // bib
							// "&nbsp;&nbsp;&nbsp;title={Dynamic Global Memory for Document-level Argument Extraction},<br>" +
							// "&nbsp;&nbsp;&nbsp;author={Du, Xinya and Li, Sha and Ji, Heng},<br>" +
							// "&nbsp;&nbsp;&nbsp;booktitle={Association for Computational Linguistics (ACL)},<br>" +
							// "&nbsp;&nbsp;&nbsp;year={2022}<br>}",
							"Few-shot learning arises in important practical scenarios, such as when a natural language understanding system needs to learn new semantic labels for an emerging, resource-scarce domain. In this paper, we explore retrieval-based methods for intent classification and slot filling tasks in few-shot settings. Retrieval-based methods make predictions based on labeled examples in the retrieval index that are similar to the input, and thus can adapt to new domains simply by changing the index without having to retrain the model. However, it is non-trivial to apply such methods on tasks with a complex label space like slot filling. To this end, we propose a span-level retrieval method that learns similar contextualized representations for spans with the same label via a novel batch-softmax objective. At inference time, we use the labels of the retrieved spans to construct the final structure with the highest aggregated score. Our method outperforms previous systems in various few-shot settings on the CLINC and SNIPS benchmarks.", // abstract
							"https://arxiv.org/abs/2104.05763", // arXiv_link
							// "https://github.com/xinyadu/grit_doc_event_entity", // code
						)
						
						add_paper("QA-Driven Zero-shot Slot Filling with Weak Supervision Pretraining",
							"<strong>Xinya Du</strong>, Luheng He, Qi Li, Dian Yu, Panupong Pasupat and Yuan Zhang", // authors
							"ACL 2021", // conference
							"https://aclanthology.org/2021.acl-short.83.pdf", // link
							null,
							// "@InProceedings{du2022dynamic,<br>" + // bib
							// "&nbsp;&nbsp;&nbsp;title={Dynamic Global Memory for Document-level Argument Extraction},<br>" +
							// "&nbsp;&nbsp;&nbsp;author={Du, Xinya and Li, Sha and Ji, Heng},<br>" +
							// "&nbsp;&nbsp;&nbsp;booktitle={Association for Computational Linguistics (ACL)},<br>" +
							// "&nbsp;&nbsp;&nbsp;year={2022}<br>}",
							"Slot-filling is an essential component for build- ing task-oriented dialog systems. In this work, we focus on the zero-shot slot-filling problem, where the model needs to predict slots and their values, given utterances from new do- mains without training on the target domain. Prior methods directly encode slot descrip- tions to generalize to unseen slot types. How- ever, raw slot descriptions are often ambigu- ous and do not encode enough semantic in- formation, limiting the models’ zero-shot ca- pability. To address this problem, we intro- duce QA-driven slot filling (QASF), which ex- tracts slot-filler spans from utterances with a span-based QA model. We use a linguistically motivated questioning strategy to turn descrip- tions into questions, allowing the model to gen- eralize to unseen slot types. Moreover, our QASF model can benefit from weak supervi- sion signals from QA pairs synthetically gen- erated from unlabeled conversations. Our full system substantially outperforms baselines by over 5% on the SNIPS benchmark.", // abstract
							// "https://arxiv.org/abs/2104.05763", // arXiv_link
							// "https://github.com/xinyadu/grit_doc_event_entity", // code
						)

						add_paper("Event Extraction by Answering (Almost) Natural <strong>Questions</strong>",
							"<b>Xinya Du</b>, Claire Cardie", // authors
							"EMNLP 2020", // conference
							"https://aclanthology.org/2020.emnlp-main.49/", // link
							// null,
							"@Inproceedings{du2022eeqa,<br>" + // bib
							"&nbsp;&nbsp;&nbsp;title={Event Extraction by Answering (Almost) Natural Questions},<br>" +
							"&nbsp;&nbsp;&nbsp;author={Du, Xinya and Cardie, Claire},<br>" +
							"&nbsp;&nbsp;&nbsp;booktitle={EMNLP)},<br>" +
							"&nbsp;&nbsp;&nbsp;year={2022}<br>}",
							"The problem of event extraction requires detecting the event trigger and extracting its corresponding arguments. Existing work in event argument extraction typically relies heavily on entity recognition as a preprocessing/concurrent step, causing the well-known problem of error propagation. To avoid this issue, we introduce a new paradigm for event extraction by formulating it as a question answering (QA) task that extracts the event arguments in an end-to-end manner. Empirical results demonstrate that our framework outperforms prior methods substantially; in addition, it is capable of extracting event arguments for roles not seen at training time (i.e., in a zero-shot learning setting).", // abstract
							"https://arxiv.org/abs/2004.13625", // arXiv_link
							"https://github.com/xinyadu/eeqa", // code
						)

						add_paper("Improving Event Duration Prediction via Time-aware Pre-training",
							"Zonglin Yang, <strong>Xinya Du</strong>, Alexander Rush, Claire Cardie", // authors
							"EMNLP 2020 (Findings)", // conference
							"https://aclanthology.org/2020.findings-emnlp.302.pdf", // link
							null,
							// "@Inproceedings{du2022eeqa,<br>" + // bib
							// "&nbsp;&nbsp;&nbsp;title={Dynamic Global Memory for Document-level Argument Extraction},<br>" +
							// "&nbsp;&nbsp;&nbsp;author={Du, Xinya and Cardie, Claire},<br>" +
							// "&nbsp;&nbsp;&nbsp;booktitle={EMNLP)},<br>" +
							// "&nbsp;&nbsp;&nbsp;year={2022}<br>}",
							"End-to-end models in NLP rarely encode external world knowledge about length of time. We introduce two effective models for duration prediction, which incorporate external knowledge by reading temporal-related news sentences (time-aware pre-training). Specifically, one model predicts the range/unit where the duration value falls in (R-PRED); and the other predicts the exact duration value (E-PRED). Our best model – E-PRED, substantially outperforms previous work, and captures duration information more accurately than R-PRED. We also demonstrate our models are capable of duration prediction in the unsupervised setting, outperforming the baselines.", // abstract
							// "https://arxiv.org/abs/2004.13625", // arXiv_link
							// "https://github.com/xinyadu/eeqa", // code
						)

						add_paper("Document-Level <em>Event Role Filler</em> Extraction Using Multi-Granularity Contextualized Encoding",
							"<strong>Xinya Du</strong>, Claire Cardie", // authors
							"ACL 2020", // conference
							"https://aclanthology.org/2020.acl-main.714/", // link
							// null,
							"@Inproceedings{du2020document,<br>" + // bib
							"&nbsp;&nbsp;&nbsp;title={Document-Level Event Role Filler Extraction Using Multi-Granularity Contextualized Encoding},<br>" +
							"&nbsp;&nbsp;&nbsp;author={Du, Xinya and Cardie, Claire},<br>" +
							"&nbsp;&nbsp;&nbsp;booktitle={ACL},<br>" +
							"&nbsp;&nbsp;&nbsp;year={2020}<br>}",
							"Few works in the literature of event extraction have gone beyond individual sentences to make extraction decisions. This is problematic when the information needed to recognize an event argument is spread across multiple sentences. We argue that document-level event extraction is a difficult task since it requires a view of a larger context to determine which spans of text correspond to event role fillers. We first investigate how end-to-end neural sequence models (with pre-trained language model representations) perform on document-level role filler extraction, as well as how the length of context captured affects the models’ performance. To dynamically aggregate information captured by neural representations learned at different levels of granularity (e.g., the sentence- and paragraph-level), we propose a novel multi-granularity reader. We evaluate our models on the MUC-4 event extraction dataset, and show that our best system performs substantially better than prior work. We also report findings on the relationship between context length and neural model performance on the task.", // abstract
							null,
							// "https://arxiv.org/abs/2004.13625", // arXiv_link
							"https://github.com/xinyadu/doc_event_role", // code
						)


						add_paper("Be Consistent! Improving Procedural Text Comprehension using Label Consistency",
							"<b>Xinya Du</b>, Bhavana Dalvi, Niket Tandon, Antoine Bosselut, Wen-tau Yih, Peter Clark, Claire Cardie", // authors
							"NAACL 2019", // conference
							"https://aclanthology.org/N19-1244/", // link
							// null,
							"@Inproceedings{du2019consistency,<br>" + // bib
							"&nbsp;&nbsp;&nbsp;title={Be Consistent! Improving Procedural Text Comprehension using Label Consistency},<br>" +
							"&nbsp;&nbsp;&nbsp;author={Du, Xinya  and Dalvi, Bhavana  and Tandon, Niket  and Bosselut, Antoine  and Yih, Wen-tau  and Clark, Peter  and Cardie, Claire},<br>" +
							"&nbsp;&nbsp;&nbsp;booktitle={NAACL},<br>" +
							"&nbsp;&nbsp;&nbsp;year={2019}<br>}",
							"Our goal is procedural text comprehension, namely tracking how the properties of entities (e.g., their location) change with time given a procedural text (e.g., a paragraph about photosynthesis, a recipe). This task is challenging as the world is changing throughout the text, and despite recent advances, current systems still struggle with this task. Our approach is to leverage the fact that, for many procedural texts, multiple independent descriptions are readily available, and that predictions from them should be consistent (label consistency). We present a new learning framework that leverages label consistency during training, allowing consistency bias to be built into the model. Evaluation on a standard benchmark dataset for procedural text, ProPara (Dalvi et al., 2018), shows that our approach significantly improves prediction performance (F1) over prior state-of-the-art systems.", // abstract
							null,
							// "https://arxiv.org/abs/2004.13625", // arXiv_link
							// "https://github.com/xinyadu/doc_event_role", // code
						)

						add_paper("Harvesting Paragraph-level Question-Answer Pairs from Wikipedia",
							"<b>Xinya Du</b>, Claire Cardie", // authors
							"ACL 2018", // conference
							"https://aclanthology.org/P18-1177/", // link
							// null,
							"@Inproceedings{du2018harvesting,<br>" + // bib
							"&nbsp;&nbsp;&nbsp;title={Harvesting Paragraph-level Question-Answer Pairs from Wikipedia},<br>" +
							"&nbsp;&nbsp;&nbsp;author={Du, Xinya and Cardie, Claire},<br>" +
							"&nbsp;&nbsp;&nbsp;booktitle={ACL},<br>" +
							"&nbsp;&nbsp;&nbsp;year={2018}<br>}",
							"We study the task of generating from Wikipedia articles question-answer pairs that cover content beyond a single sentence. We propose a neural network approach that incorporates coreference knowledge via a novel gating mechanism. As compared to models that only take into account sentence-level information (Heilman and Smith, 2010; Du et al., 2017; Zhou et al., 2017), we find that the linguistic knowledge introduced by the coreference representation aids question generation significantly, producing models that outperform the current state-of-the-art. We apply our system (composed of an answer span extraction system and the passage-level QG system) to the 10,000 top ranking Wikipedia articles and create a corpus of over one million question-answer pairs. We provide qualitative analysis for the this large-scale generated corpus from Wikipedia.", // abstract
							// null,
							"https://arxiv.org/pdf/1805.05942.pdf", // arXiv_link
							"https://github.com/xinyadu/harvestingQA", // code
						)

						add_paper("<strong>Learning to Ask</strong>: Neural Question Generation for Reading Comprehension",
							"<b>Xinya Du</b>, Junru Shao, Claire Cardie", // authors
							"ACL 2017", // conference
							"https://aclanthology.org/P17-1123/", // link
							// null,
							"@Inproceedings{du2017learning,<br>" + // bib
							"&nbsp;&nbsp;&nbsp;title={Learning to Ask: Neural Question Generation for Reading Comprehension},<br>" +
							"&nbsp;&nbsp;&nbsp;author={Du, Xinya  and Shao, Junru  and Cardie, Claire},<br>" +
							"&nbsp;&nbsp;&nbsp;booktitle={ACL},<br>" +
							"&nbsp;&nbsp;&nbsp;year={2017}<br>}",
							"We study the task of generating from Wikipedia articles question-answer pairs that cover content beyond a single sentence. We propose a neural network approach that incorporates coreference knowledge via a novel gating mechanism. As compared to models that only take into account sentence-level information (Heilman and Smith, 2010; Du et al., 2017; Zhou et al., 2017), we find that the linguistic knowledge introduced by the coreference representation aids question generation significantly, producing models that outperform the current state-of-the-art. We apply our system (composed of an answer span extraction system and the passage-level QG system) to the 10,000 top ranking Wikipedia articles and create a corpus of over one million question-answer pairs. We provide qualitative analysis for the this large-scale generated corpus from Wikipedia.", // abstract
							// null,
							"https://arxiv.org/abs/1705.00106", // arXiv_link
							"https://github.com/xinyadu/nqg", // code
							null, //data
							"files/acl17_dsc_poster.pdf", //slides
							null, //talk
							"<img src=\"imgs/tv-icon.png\" width=\"30px\" /> Featured in <a href=\"https://www.newscientist.com/article/2130205-inquisitive-bot-asks-questions-to-test-your-understanding/\"><em>New Scientist</em></a> <a><img src=\"imgs/newscientist.jpg\" width=\"40px\" /></a> <a href=\"http://www.techrepublic.com/article/how-researchers-trained-one-ai-system-to-start-asking-its-own-questions/\"><em>Tech Republic</em></a> <a><img src=\"imgs/techrepublic.png\" width=\"70px\" /></a><br>" +
    						"<a href=\"https://www.paperdigest.org/2022/05/most-influential-acl-papers-2022-05/\" style=\"color: red;\"><strong>🏆 Included in ACL Most Influential Papers (15 each year, 1\%)</strong></a><br>"
							// <a href='https://www.paperdigest.org/2022/05/most-influential-acl-papers-2022-05/'>
						)
						add_paper("Identifying Where to Focus in Reading Comprehension for Neural Question Generation",
							"<b>Xinya Du</b>, Claire Cardie", // authors
							"EMNLP 2017", // conference
							"https://aclanthology.org/D17-1219/", // link
							// null,
							"@Inproceedings{du2017identifying,<br>" + // bib
							"&nbsp;&nbsp;&nbsp;title={Identifying Where to Focus in Reading Comprehension for Neural Question Generation},<br>" +
							"&nbsp;&nbsp;&nbsp;author={Du, Xinya and Cardie, Claire},<br>" +
							"&nbsp;&nbsp;&nbsp;booktitle={EMNLP},<br>" +
							"&nbsp;&nbsp;&nbsp;year={2017}<br>}",
							"A first step in the task of automatically generating questions for testing reading comprehension is to identify question-worthy sentences, i.e. sentences in a text passage that humans find it worthwhile to ask questions about. We propose a hierarchical neural sentence-level sequence tagging model for this task, which existing approaches to question generation have ignored. The approach is fully data-driven — with no sophisticated NLP pipelines or any hand-crafted rules/features — and compares favorably to a number of baselines when evaluated on the SQuAD data set. When incorporated into an existing neural question generation system, the resulting end-to-end system achieves state-of-the-art performance for paragraph-level question generation for reading comprehension.", // abstract
							null,
							// "https://arxiv.org/abs/1705.00106", // arXiv_link
							null,
							// "https://github.com/xinyadu/nqg", // code
							null, //data

							// "./files/acl17_dsc_poster.pdf", //slides
							null, //talk
						)
						document.write("</ul>")

					</script>
				</div>

				<div id="allPublicationList" class="publication-list mt-3">
					Please check my <a href="https://scholar.google.com/citations?hl=en&user=R-lKQqkAAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a> page and my <a href="https://dblp.org/pid/200/8114.html">DBLP</a> page.
				</div>

				<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
			</section>

		</div>
	</body>


	