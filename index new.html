index new

<li> <em><b>Trustworthy Knowledge and Reasoning Capability:</b> How do we enable NLP systems / LLMs to conduct <a href="https://www.amazon.science/research-awards/recipients/xinya-du">faithful</a> and explainable reasoning across <a href="https://arxiv.org/pdf/2311.01477.pdf">modalities</a>?</em>

    Specifically, how do we leverage <a href="https://aclanthology.org/2022.acl-long.361/">contextual and external knowledge</a> into end-to-end models to improve <a href="https://arxiv.org/abs/2208.14271">faithful</a> and factual behaviors?  

How can we exploit models to <a href="https://arxiv.org/pdf/2212.10923.pdf">induce new rules</a> and <a href="https://arxiv.org/abs/2309.02726">hypotheses</a>, and understand the <a href="https://w.sentic.net/commonsense-knowledge-base-completion.pdf">reasoning</a> capabilities of large pre-trained models?  

How do we comprehensively <a href="https://arxiv.org/pdf/2307.02762.pdf">evaluate</a> reasoning capability and transparency of explanations, ensuring alignment with <a href="https://arxiv.org/pdf/2408.13545">human judgements</a>?  

</li> <br> <li> <em><b>Alignment, Safety, and Trustworthy AI:</b></em>

    How do we design evaluation and alignment techniques to ensure LLMs behave safely and transparently?  

My work on <a href="https://arxiv.org/abs/2408.14033">IQA-Eval</a>, <a href="https://arxiv.org/abs/2309.02726">PRD</a>, and the <a href="https://www.amazon.science/nova-ai-challenge/proceedings/comet-closed-loop-orchestration-for-malicious-elicitation-techniques-in-code-models">Nova AI Safety Challenge</a> develops frameworks for assessing trustworthiness, robustness, and adversarial resilience of LLMs.  

I also investigate methods for reducing hallucinations and ensuring <a href="https://arxiv.org/abs/2408.13545">faithfulness</a> through fine-grained evaluation (<a href="https://neurips.cc/virtual/2024/poster/95926">FaithScore</a>, FG-PRM, FGAIF).  

A key focus is creating mechanisms for <b>automatic feedback collection</b> (e.g., FGAIF, FG-PRM) and <b>AI constitutions</b>, enabling models to self-correct and align with human values.  


</li> <br> <li> <em><b>NLP and Science / Multimodality / Humanâ€“computer interaction:</b></em>

    How do we design LLM-based frameworks for <a href="https://arxiv.org/abs/2309.02726">generating</a> scientific hypotheses, <a href="https://www.arxiv.org/abs/2408.14033">implementing and executing</a> scientific experiments in an <a href="https://www.nature.com/articles/s41586-023-06792-0">autonomous</a> way?  

How do we design NLP techniques across <a href="https://llava-vl.github.io">modalities</a> and for <a href="https://arxiv.org/pdf/2307.03073.pdf">interdisciplinary</a> research?  

How do we better align LLMs with human values, enabling both models and humans to understand <a href="https://github.com/Mooler0410/LLMsPracticalGuide#Usage-and-Restrictions">community policies</a> and behave accordingly?  
